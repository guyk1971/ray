{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rllib Usage flows\n",
    "This notebook demonstrate various Rllib usage flows, beyond the basic ones\n",
    "The goal is to be able to train an agent and then use its policy to do inference.\n",
    "\n",
    "We want to be able to save trainer weights during training and then load it and do the inference.\n",
    "\n",
    "## Basic flows\n",
    "we've seen in the [Rllib docs](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) that we can run rllib from command line:\n",
    "```\n",
    "rllib train --run DQN --env CartPole-v0  # --eager [--trace] for eager execution\n",
    "\n",
    "rllib rollout \\\n",
    "    ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "```\n",
    "\n",
    "Also, we can run the training from within python (see [Rllib Training APIs](https://ray.readthedocs.io/en/latest/rllib-training.html#rllib-training-apis)):\n",
    "\n",
    "![rllib-api](./rllib-intro/rllib-api.svg)\n",
    "\n",
    "Although there is a direct access to manually call the trainer's `train` method (see [this](https://ray.readthedocs.io/en/latest/rllib-training.html#basic-python-api)) , \n",
    "it is recommended to call it through Tune as in the following example:\n",
    "```\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "        \"eager\": False,\n",
    "    },\n",
    ")\n",
    "``` \n",
    "here we provided the trainer as string \"PPO\". we were able to do so because PPO is already registered.\n",
    "\n",
    "Alternatively, we could define a trainer and send it to tune directly - as done in [custom_tf_policy.py](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py):\n",
    "```\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss,\n",
    "    postprocess_fn=calculate_advantages,\n",
    ")\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "MyTrainer = build_trainer(\n",
    "    name=\"MyCustomTrainer\",\n",
    "    default_policy=MyTFPolicy,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    ray.init(num_cpus=args.num_cpus or None)\n",
    "    tune.run(\n",
    "        MyTrainer,\n",
    "        stop={\"training_iteration\": args.iters},\n",
    "        config={\n",
    "            \"env\": \"CartPole-v0\",\n",
    "            \"num_workers\": 2,\n",
    "            \"num_gpus\":args.num_gpus,       # GuyK\n",
    "        })\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we could also give it a custom training function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = PPOTrainer(config=config, env=YourEnv)\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "        if result[\"episode_reward_mean\"] > 200:\n",
    "            phase = 2\n",
    "        elif result[\"episode_reward_mean\"] > 100:\n",
    "            phase = 1\n",
    "        else:\n",
    "            phase = 0\n",
    "        trainer.workers.foreach_worker(\n",
    "            lambda ev: ev.foreach_env(\n",
    "                lambda env: env.set_phase(phase)))\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    train,\n",
    "    config={\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": lambda spec: spec.config.num_gpus,\n",
    "        \"extra_cpu\": lambda spec: spec.config.num_workers,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "All RLlib trainers are compatible with the Tune API. This enables them to be easily used in experiments with Tune. \n",
    "They inherit from a base class called `Trainable` who's [API](https://ray.readthedocs.io/en/latest/tune-usage.html#trainable-api) allows advanced operations\n",
    "\n",
    "in the following, we'll try to form a flow that will allow the following:\n",
    "1. train an agent and save to file\n",
    "2. load a policy model from file and do inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training flow with checkpoints\n",
    "in this section, we'll form a flow that uses [Tune Training API](https://ray.readthedocs.io/en/latest/tune-usage.html#tune-training-api) and saves a checkpoint of the trainer s.t. we can later load and evaluate. \n",
    "\n",
    "Lets assume that we have in the config file some parameters that are related to saving checkpoints.\n",
    "1. we can save checkpoint synchronously - e.g. every X timesteps in the environment\n",
    "2. we can save checkpoint asynchronously - e.g. when we break a record in evaluation score (be it on simulation or using OPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like it should be via callback. \n",
    "I should read a `.yaml` file as configuration. not sure it can be consumed directly by rllib. \n",
    "in any case, I will digest it and prepare a config dict to send to tune.\n",
    "\n",
    "The script will be implemented in <font color='blue'>**train_w_tune.py**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some background info \n",
    "in this script we'll still use the registry for both the agent and environment.\n",
    "we'll try to use the call backs to save checkpoints.  \n",
    "There are several possible callbacks (see [callbacks and custom metrics](https://ray.readthedocs.io/en/latest/rllib-training.html#callbacks-and-custom-metrics)):\n",
    "```\n",
    "analysis = tune.run(\n",
    "    \"PG\",\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"callbacks\": {\n",
    "            \"on_episode_start\": on_episode_start,\n",
    "            \"on_episode_step\": on_episode_step,\n",
    "            \"on_episode_end\": on_episode_end,\n",
    "            \"on_train_result\": on_train_result,\n",
    "            \"on_postprocess_traj\": on_postprocess_traj,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "```\n",
    "it looks like the keys of the `callbacks` are specific predefined locations in the agent's code that it checks whether there is a callback and if there is, it calls it from there.\n",
    "\n",
    "There are 3 questions here:\n",
    "1. what are all the possible callbacks that tune supports\n",
    "2. where are they located in the code\n",
    "3. what is the information that we can get in the callback ? do we have access to the trainer s.t. we can invoke `trainer.save()` ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are all the possible callbacks\n",
    "we build the trainer with `build_trainer` function. this function builds a `Trainer` object.  \n",
    "This `Trainer` is defined in [trainer.py](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py). \n",
    "in there, you can also find the defult configuration dict (called [`COMMON_CONFIG`](https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters)). \n",
    "in this dictionary one can find the possible callbacks:\n",
    "```\n",
    "    \"callbacks\": {\n",
    "        \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}\n",
    "        \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}\n",
    "        \"on_postprocess_traj\": None,  # arg: {\n",
    "                                      #   \"agent_id\": ..., \"episode\": ...,\n",
    "                                      #   \"pre_batch\": (before processing),\n",
    "                                      #   \"post_batch\": (after processing),\n",
    "                                      #   \"all_pre_batches\": (other agent ids),\n",
    "                                      # }\n",
    "    },\n",
    "```\n",
    "in addition, it looks like there are additional 'events' given in the `build_trainer` function. for example, if we look at the [`dqn.py`](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/dqn.py) file:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "```\n",
    "we see that there are `before_train_step`, `after_optimizer_step` and `after_train_result`. what are these ? \n",
    "in [`trainer_template.py`](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer_template.py) we can find the documentation of these functions:\n",
    "- **after_init** (func) : optional function to run at the end of trainer init that takes the trainer instance as argument  \n",
    "- **before_train_step** (func): optional callback to run **before each train() call**. It takes the trainer instance as an argument.\n",
    "- **after_optimizer_step** (func): optional callback to run after each step() call to the policy optimizer. It takes the trainer instance and the policy gradient fetches as arguments.  \n",
    "- **after_train_result** (func): optional callback to run at the end of each train() call. It takes the trainer instance and result dict as arguments, and may mutate the result dict as needed.  \n",
    "- **collect_metrics_fn** (func): override the method used to collect metrics. It takes the trainer instance as argumnt.  \n",
    "- **before_evaluate_fn** (func): callback to run before evaluation. This takes the trainer instance as argument.\n",
    "\n",
    "all these callback functions are called from within the `_train` implementation of the `trainer_cls` that is built (see note below).\n",
    "\n",
    "> **Note** the callback function in the configuration all get `info` dict as an argument. the functions defined in the `build_trainer` take the trainer instance as an argument. we need to understand what is this `info` dict but it looks like it wont be enough to save a checkpoint. It looks like the more (or only) appropriate place to do it is in the `after_train_result`.\n",
    "\n",
    "\n",
    "> Note that the `build_trainer` function takes a `Trainer` class as a base class, add some `mixins` and define a subclass `trainer_cls` which is returned to the caller of `build_trainer`.  \n",
    "> Note that `trainer_cls` has implementation of both `__init__` and `_init`.  the `_init` is called from `_setup` of the parent class (`Trainer`). and the `Trainer._setup` is called from the `Trainer`'s parent class (`Trainable`) during its `__init__`. \n",
    "\n",
    "if we look at the documentation on [Contributing to Rllib](https://ray.readthedocs.io/en/latest/rllib-dev.html#contributing-to-rllib) we see that they describe how to create an agent:  \n",
    "\"*It takes just two changes to add an algorithm to contrib. A minimal example can be found here. First, subclass `Trainer` and implement the `_init` and `_train` methods*\"  \n",
    "and this is exactly what `build_trainer` does. so we'll use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the callbacks are related ?\n",
    "so we see we have 2 ways to define callbacks. what is the relation between them ?\n",
    "Note that the call to `tune.run` can be done without having access to the trainer definition. i.e. we can `pip install ray` and write a script that doesnt need to change the code of the trainer in order to define the callback.\n",
    "\n",
    "#### `on_train_result` vs `after_train_result`\n",
    "`on_train_result` is the callback that `tune.run` gets in the `config['callbacks']`. we can see that we can define this function without having access to the trainer definition code. we can use a registered trainer (\"PG\", \"DQN\", \"PPO\") and this will use the trainer registered with these strings. in the above example we used \"PG\", but we could have used \"DQN\" and then it would use `DQNTrainer` that is defined in the rllib `dqn.py`:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "\n",
    "DQNTrainer = GenericOffPolicyTrainer.with_updates(\n",
    "    name=\"DQN\", default_policy=DQNTFPolicy, default_config=DEFAULT_CONFIG)\n",
    "```\n",
    "\n",
    "Note that within this `dqn.py` we define the `DQNTrainer` with a function called `after_train_result`. which is described in the following\n",
    "\n",
    "\n",
    "`after_train_result` \n",
    "if we build a custom trainer using the `build_trainer`, we could also define the `after_train_result` callback by ourselves and then do whatever we need there. \n",
    "\n",
    "\n",
    "**So when each of these routines is called ?**\n",
    "\n",
    "Let's follow the `train()` method. \n",
    "as we described, the `build_trainer` returns a `trainer_cls` which is child of `Trainer` and implements the `_train()` method.\n",
    "when we provide tune with a trainer (e.g. \"DQN\") , it invokes its `train()` method. \n",
    "this `train()` is implemented in `Trainer` :\n",
    "\n",
    "```\n",
    "    @override(Trainable)\n",
    "    @PublicAPI\n",
    "    def train(self):\n",
    "        \"\"\"Overrides super.train to synchronize global vars.\"\"\"\n",
    "        ...\n",
    "        result = None\n",
    "        for _ in range(1 + MAX_WORKER_FAILURE_RETRIES):\n",
    "            try:\n",
    "                result = Trainable.train(self)\n",
    "            except RayError as e:\n",
    "                ...\n",
    "\n",
    "        if hasattr(self, \"workers\") and isinstance(self.workers, WorkerSet):\n",
    "            self._sync_filters_if_needed(self.workers)\n",
    "\n",
    "        ...\n",
    "        \n",
    "        if self.config[\"evaluation_interval\"] == 1 or (\n",
    "                self._iteration > 0 and self.config[\"evaluation_interval\"]\n",
    "                and self._iteration % self.config[\"evaluation_interval\"] == 0):\n",
    "            evaluation_metrics = self._evaluate()\n",
    "            assert isinstance(evaluation_metrics, dict), \\\n",
    "                \"_evaluate() needs to return a dict.\"\n",
    "            result.update(evaluation_metrics)\n",
    "\n",
    "        return result\n",
    "\n",
    "```\n",
    "so we see a call to the parent `Trainable.train()` method with the instance of the `Trainer`. let's look at this function:\n",
    "```\n",
    "    def train(self):\n",
    "        \"\"\"Runs one logical iteration of training.\n",
    "\n",
    "        Subclasses should override ``_train()`` instead to return results.\n",
    "        ...\n",
    "\n",
    "        Returns:\n",
    "            A dict that describes training progress.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        result = self._train()\n",
    "        assert isinstance(result, dict), \"_train() needs to return a dict.\"\n",
    "\n",
    "        # We do not modify internal state nor update this result if duplicate.\n",
    "        if RESULT_DUPLICATE in result:\n",
    "            return result\n",
    "\n",
    "        result = result.copy()\n",
    "\n",
    "        self._iteration += 1\n",
    "        self._iterations_since_restore += 1\n",
    "\n",
    "        if result.get(TIME_THIS_ITER_S) is not None:\n",
    "            time_this_iter = result[TIME_THIS_ITER_S]\n",
    "        else:\n",
    "            time_this_iter = time.time() - start\n",
    "        self._time_total += time_this_iter\n",
    "        self._time_since_restore += time_this_iter\n",
    "\n",
    "        result.setdefault(DONE, False)\n",
    "\n",
    "        # self._timesteps_total should only be tracked if increments provided\n",
    "        if result.get(TIMESTEPS_THIS_ITER) is not None:\n",
    "            if self._timesteps_total is None:\n",
    "                self._timesteps_total = 0\n",
    "            self._timesteps_total += result[TIMESTEPS_THIS_ITER]\n",
    "            self._timesteps_since_restore += result[TIMESTEPS_THIS_ITER]\n",
    "\n",
    "        # self._episodes_total should only be tracked if increments provided\n",
    "        if result.get(EPISODES_THIS_ITER) is not None:\n",
    "            if self._episodes_total is None:\n",
    "                self._episodes_total = 0\n",
    "            self._episodes_total += result[EPISODES_THIS_ITER]\n",
    "\n",
    "        # self._timesteps_total should not override user-provided total\n",
    "        result.setdefault(TIMESTEPS_TOTAL, self._timesteps_total)\n",
    "        result.setdefault(EPISODES_TOTAL, self._episodes_total)\n",
    "        result.setdefault(TRAINING_ITERATION, self._iteration)\n",
    "\n",
    "        # Provides auto-filled neg_mean_loss for avoiding regressions\n",
    "        if result.get(\"mean_loss\"):\n",
    "            result.setdefault(\"neg_mean_loss\", -result[\"mean_loss\"])\n",
    "\n",
    "        now = datetime.today()\n",
    "        result.update(\n",
    "            experiment_id=self._experiment_id,\n",
    "            date=now.strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "            timestamp=int(time.mktime(now.timetuple())),\n",
    "            time_this_iter_s=time_this_iter,\n",
    "            time_total_s=self._time_total,\n",
    "            pid=os.getpid(),\n",
    "            hostname=os.uname()[1],\n",
    "            node_ip=self._local_ip,\n",
    "            config=self.config,\n",
    "            time_since_restore=self._time_since_restore,\n",
    "            timesteps_since_restore=self._timesteps_since_restore,\n",
    "            iterations_since_restore=self._iterations_since_restore)\n",
    "\n",
    "        monitor_data = self._monitor.get_data()\n",
    "        if monitor_data:\n",
    "            result.update(monitor_data)\n",
    "\n",
    "        self._log_result(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "```\n",
    "\n",
    "There are 2 important things to note:\n",
    "1. at the beginning there is a call to `result = self._train()` that is implemented by the subclass\n",
    "2. at the end there is a call to `self._log_result(result)` \n",
    "\n",
    "\n",
    "first, let's look into `self._train()` that is implemented in the `trainer_cls` :\n",
    "```\n",
    "        def _train(self):\n",
    "            if self.train_exec_impl:\n",
    "                return self._train_exec_impl()\n",
    "\n",
    "            if before_train_step:\n",
    "                before_train_step(self)\n",
    "            prev_steps = self.optimizer.num_steps_sampled\n",
    "\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                fetches = self.optimizer.step()\n",
    "                if after_optimizer_step:\n",
    "                    after_optimizer_step(self, fetches)\n",
    "                if (time.time() - start >= self.config[\"min_iter_time_s\"]\n",
    "                        and self.optimizer.num_steps_sampled - prev_steps >=\n",
    "                        self.config[\"timesteps_per_iteration\"]):\n",
    "                    break\n",
    "\n",
    "            if collect_metrics_fn:\n",
    "                res = collect_metrics_fn(self)\n",
    "            else:\n",
    "                res = self.collect_metrics()\n",
    "            res.update(\n",
    "                timesteps_this_iter=self.optimizer.num_steps_sampled -\n",
    "                prev_steps,\n",
    "                info=res.get(\"info\", {}))\n",
    "\n",
    "            if after_train_result:\n",
    "                after_train_result(self, res)\n",
    "            return res\n",
    "\n",
    "```\n",
    "we see that within this method, we call the callbacks provided as argument to `build_trainer`. specifically, see at the bottom the call to `after_train_result(self,res)`. \n",
    "\n",
    "\n",
    "\n",
    "second, if we look at the line `self._log_result(result)` towards the end of `train()` implementation (in the `Trainer`). it calls the following `Trainer._log_result()`:\n",
    "```\n",
    "    @override(Trainable)\n",
    "    def _log_result(self, result):\n",
    "        if self.config[\"callbacks\"].get(\"on_train_result\"):\n",
    "            self.config[\"callbacks\"][\"on_train_result\"]({\n",
    "                \"trainer\": self,\n",
    "                \"result\": result,\n",
    "            })\n",
    "        # log after the callback is invoked, so that the user has a chance\n",
    "        # to mutate the result\n",
    "        Trainable._log_result(self, result)\n",
    "```\n",
    "we see that it calls the callback we have defined in the `config['Callbacks']` argument to `tune.run()`.\n",
    "\n",
    "in this manner we can track each of the callbacks provided to `tune.run` and see at which point they are called. \n",
    "\n",
    "**Bottom line**\n",
    "we should strive to define the model/checkpoint store using the callbacks provided to `tune.run()` s.t. we dont have to define a callback per each trainer (e.g. DQN, PG, PPO etc.). \n",
    "if we were defining it as input to `build_trainer` we should have implemented the callback in each of the trainers definition files (dqn.py, ppo.py etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while tracking the `save` method of the trainer, I noticed that it's base class (`Trainable`) has method `export_model`.\n",
    "This method calls an internal `_export_model` method that should be implemented by `Trainable` subclass. \n",
    "and it is indeed implemented in rllib's `Trainer` class.\n",
    "```\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        ExportFormat.validate(export_formats)\n",
    "        exported = {}\n",
    "        if ExportFormat.CHECKPOINT in export_formats:\n",
    "            path = os.path.join(export_dir, ExportFormat.CHECKPOINT)\n",
    "            self.export_policy_checkpoint(path)\n",
    "            exported[ExportFormat.CHECKPOINT] = path\n",
    "        if ExportFormat.MODEL in export_formats:\n",
    "            path = os.path.join(export_dir, ExportFormat.MODEL)\n",
    "            self.export_policy_model(path)\n",
    "            exported[ExportFormat.MODEL] = path\n",
    "        return exported\n",
    "```\n",
    "\n",
    "as we can see it calls `self.export_policy_checkpoint` and `self.export_policy_model`. \n",
    "I think that as the trainer has multiple workers, this is the place to call on each worker to save its policy instance. \n",
    "I guess that this function will use the policy API to save the checkpoint. \n",
    "\n",
    "the interaction between the `Trainer` object and the `policy` object is that the trainer manages the workers and each has a policy object to run with. \n",
    "see for example the implementation of `export_policy_checkpoint` in the Trainer: \n",
    "```\n",
    "    @DeveloperAPI\n",
    "    def export_policy_model(self, export_dir, policy_id=DEFAULT_POLICY_ID):\n",
    "        \"\"\"Export policy model with given policy_id to local directory.\n",
    "\n",
    "        self.workers.local_worker().export_policy_model(export_dir, policy_id)\n",
    "\n",
    "    @DeveloperAPI\n",
    "    def export_policy_checkpoint(self,\n",
    "                                 export_dir,\n",
    "                                 filename_prefix=\"model\",\n",
    "                                 policy_id=DEFAULT_POLICY_ID):\n",
    "        \"\"\"Export tensorflow policy model checkpoint to local directory.\n",
    "\n",
    "        self.workers.local_worker().export_policy_checkpoint(\n",
    "            export_dir, filename_prefix, policy_id)\n",
    "```\n",
    "\n",
    "Question: how does `export_policy_model` and `export_policy_checkpoint` is implemented ?\n",
    "we see that the trainer calls the `local_worker` method on its `workers` member (`WorkerSet` type). if we look at `WorkerSet` implmenetation ([worker_set.py](https://github.com/ray-project/ray/blob/master/rllib/evaluation/worker_set.py))we see that the `local_worker` is a `Rolloutworker` :\n",
    "```\n",
    "# in  WorkerSet.__init__:\n",
    "    # Always create a local worker\n",
    "    self._local_worker = self._make_worker(\n",
    "        RolloutWorker, env_creator, policy, 0, self._local_config)\n",
    "```\n",
    "\n",
    "and this `RolloutWorker` is implemented in [rollout_worker.py](https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py) and has a method that calls to its policy object to export the model:\n",
    "\n",
    "```\n",
    "    @DeveloperAPI\n",
    "    def export_policy_model(self, export_dir, policy_id=DEFAULT_POLICY_ID):\n",
    "        self.policy_map[policy_id].export_model(export_dir)\n",
    "\n",
    "```\n",
    "\n",
    "in a very similar way the checkpoint is saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rllib Policy object\n",
    "[`CLASS ray.rllib.policy.Policy(observation_space, action_space, config)`](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#module-ray.rllib.policy)  \n",
    "This object defines how to act in the environment, and also losses used to improve the policy based on its experiences. Note that both policy and loss are defined together for convenience, though the policy itself is logically separate.\n",
    "\n",
    "All policies can directly extend Policy, however TensorFlow users may find TFPolicy simpler to implement. TFPolicy also enables RLlib to apply TensorFlow-specific optimizations such as fusing multiple policy graphs and multi-GPU support.\n",
    "\n",
    "**TODO** need to check the relation between the policy object and the policy optimizer.\n",
    "the [Rllib policy](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#module-ray.rllib.policy) object has also an `export` method. when is it called ? \n",
    "\n",
    "**TODO** look at `TFPolicy` and `build_tf_policy` to understand whether I should directly save the policy model at the end and not a checkpoint of the trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How a policy object is created ?**\n",
    "when we build a trainer using `build_trainer` we provide it with `deafult_policy` which is the policy class that we want to train.  for example, in dqn:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "\n",
    "DQNTrainer = GenericOffPolicyTrainer.with_updates(name=\"DQN\", default_policy=DQNTFPolicy, default_config=DEFAULT_CONFIG)\n",
    "```\n",
    "This policy class (e.g. `DQNTFPolicy`) can be built in 2 main ways, similar to `Trainer` :\n",
    "1. Directly inherit from the `Policy` class (or one of its descendants : `TFPolicy` &rarr; `DynamicTFPolic`)\n",
    "1. Use `build_tf_policy` to build the class. for example in dqn_policy.py:\n",
    "```\n",
    "DQNTFPolicy = build_tf_policy(\n",
    "    name=\"DQNTFPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.dqn.dqn.DEFAULT_CONFIG,\n",
    "    make_model=build_q_model,\n",
    "    action_sampler_fn=sample_action_from_q_network,\n",
    "    log_likelihood_fn=get_log_likelihood,\n",
    "    loss_fn=build_q_losses,\n",
    "    stats_fn=build_q_stats,\n",
    "    postprocess_fn=postprocess_nstep_and_prio,\n",
    "    optimizer_fn=adam_optimizer,\n",
    "    gradients_fn=clip_gradients,\n",
    "    extra_action_fetches_fn=lambda policy: {\"q_values\": policy.q_values},\n",
    "    extra_learn_fetches_fn=lambda policy: {\"td_error\": policy.q_loss.td_error},\n",
    "    before_init=setup_early_mixins,\n",
    "    before_loss_init=setup_mid_mixins,\n",
    "    after_init=setup_late_mixins,\n",
    "    obs_include_prev_action_reward=False,\n",
    "    mixins=[\n",
    "        ParameterNoiseMixin,\n",
    "        TargetNetworkMixin,\n",
    "        ComputeTDErrorMixin,\n",
    "        LearningRateSchedule,\n",
    "    ])\n",
    "```\n",
    "\n",
    "    this builder is implemented in [`tf_policy_template.py`](https://github.com/ray-project/ray/blob/master/rllib/policy/tf_policy_template.py) works very similar to `build_trainer`. it defines a base class that is constructed from `DynamicTFPolicy` and add some mixins that are provided as argument (see last argument in the above example) - and return this `policy_cls`.  \n",
    "\n",
    "All the mechanism of saving checkpoint or model is implemented in the `TFPolicy` (or the equivalent Pytorch class).\n",
    "\n",
    "What is the difference between saving a model and a checkpoint ? see [the implementation](https://ray.readthedocs.io/en/latest/_modules/ray/rllib/policy/tf_policy.html#TFPolicy.export_checkpoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming the flow\n",
    "OK, given the above, lets form the code for training while saving checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
