{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rllib Usage flows\n",
    "This notebook demonstrate various Rllib usage flows, beyond the basic ones\n",
    "The goal is to be able to train an agent and then use its policy to do inference.\n",
    "\n",
    "We want to be able to save trainer weights during training and then load it and do the inference.\n",
    "\n",
    "## Basic flows\n",
    "we've seen in the [Rllib docs](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) that we can run rllib from command line:\n",
    "```\n",
    "rllib train --run DQN --env CartPole-v0  # --eager [--trace] for eager execution\n",
    "\n",
    "rllib rollout \\\n",
    "    ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "```\n",
    "\n",
    "Also, we can run the training from within python (see [Rllib Training APIs](https://ray.readthedocs.io/en/latest/rllib-training.html#rllib-training-apis)):\n",
    "\n",
    "![rllib-api](./rllib-intro/rllib-api.svg)\n",
    "\n",
    "Although there is a direct access to manually call the trainer's `train` method (see [this](https://ray.readthedocs.io/en/latest/rllib-training.html#basic-python-api)) , \n",
    "it is recommended to call it through Tune as in the following example:\n",
    "```\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "        \"eager\": False,\n",
    "    },\n",
    ")\n",
    "``` \n",
    "here we provided the trainer as string \"PPO\". we were able to do so because PPO is already registered.\n",
    "\n",
    "Alternatively, we could define a trainer and send it to tune directly - as done in [custom_tf_policy.py](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py):\n",
    "```\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss,\n",
    "    postprocess_fn=calculate_advantages,\n",
    ")\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "MyTrainer = build_trainer(\n",
    "    name=\"MyCustomTrainer\",\n",
    "    default_policy=MyTFPolicy,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    ray.init(num_cpus=args.num_cpus or None)\n",
    "    tune.run(\n",
    "        MyTrainer,\n",
    "        stop={\"training_iteration\": args.iters},\n",
    "        config={\n",
    "            \"env\": \"CartPole-v0\",\n",
    "            \"num_workers\": 2,\n",
    "            \"num_gpus\":args.num_gpus,       # GuyK\n",
    "        })\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we could also give it a custom training function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = PPOTrainer(config=config, env=YourEnv)\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "        if result[\"episode_reward_mean\"] > 200:\n",
    "            phase = 2\n",
    "        elif result[\"episode_reward_mean\"] > 100:\n",
    "            phase = 1\n",
    "        else:\n",
    "            phase = 0\n",
    "        trainer.workers.foreach_worker(\n",
    "            lambda ev: ev.foreach_env(\n",
    "                lambda env: env.set_phase(phase)))\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    train,\n",
    "    config={\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": lambda spec: spec.config.num_gpus,\n",
    "        \"extra_cpu\": lambda spec: spec.config.num_workers,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "All RLlib trainers are compatible with the Tune API. This enables them to be easily used in experiments with Tune. \n",
    "They inherit from a base class called `Trainable` who's [API](https://ray.readthedocs.io/en/latest/tune-usage.html#trainable-api) allows advanced operations\n",
    "\n",
    "in the following, we'll try to form a flow that will allow the following:\n",
    "1. train an agent and save to file\n",
    "2. load a policy model from file and do inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training flow with checkpoints\n",
    "in this section, we'll form a flow that uses [Tune Training API](https://ray.readthedocs.io/en/latest/tune-usage.html#tune-training-api) and saves a checkpoint of the trainer s.t. we can later load and evaluate. \n",
    "\n",
    "Lets assume that we have in the config file some parameters that are related to saving checkpoints.\n",
    "1. we can save checkpoint synchronously - e.g. every X timesteps in the environment\n",
    "2. we can save checkpoint asynchronously - e.g. when we break a record in evaluation score (be it on simulation or using OPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like it should be via callback. \n",
    "I should read a `.yaml` file as configuration. not sure it can be consumed directly by rllib. \n",
    "in any case, I will digest it and prepare a config dict to send to tune.\n",
    "\n",
    "The script will be implemented in <font color='blue'>**train_w_tune.py**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some background info \n",
    "in this script we'll still use the registry for both the agent and environment.\n",
    "we'll try to use the call backs to save checkpoints.  \n",
    "There are several possible callbacks (see [callbacks and custom metrics](https://ray.readthedocs.io/en/latest/rllib-training.html#callbacks-and-custom-metrics)):\n",
    "```\n",
    "analysis = tune.run(\n",
    "    \"PG\",\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"callbacks\": {\n",
    "            \"on_episode_start\": on_episode_start,\n",
    "            \"on_episode_step\": on_episode_step,\n",
    "            \"on_episode_end\": on_episode_end,\n",
    "            \"on_train_result\": on_train_result,\n",
    "            \"on_postprocess_traj\": on_postprocess_traj,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "```\n",
    "it looks like the keys of the `callbacks` are specific predefined locations in the agent's code that it checks whether there is a callback and if there is, it calls it from there.\n",
    "\n",
    "There are 3 questions here:\n",
    "1. what are all the possible callbacks that tune supports\n",
    "2. where are they located in the code\n",
    "3. what is the information that we can get in the callback ? do we have access to the trainer s.t. we can invoke `trainer.save()` ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are all the possible callbacks\n",
    "we build the trainer with `build_trainer` function. this function builds a `Trainer` object.  \n",
    "This `Trainer` is defined in [trainer.py](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py). \n",
    "in there, you can also find the defult configuration dict (called [`COMMON_CONFIG`](https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters)). \n",
    "in this dictionary one can find the possible callbacks:\n",
    "```\n",
    "    \"callbacks\": {\n",
    "        \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}\n",
    "        \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}\n",
    "        \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}\n",
    "        \"on_postprocess_traj\": None,  # arg: {\n",
    "                                      #   \"agent_id\": ..., \"episode\": ...,\n",
    "                                      #   \"pre_batch\": (before processing),\n",
    "                                      #   \"post_batch\": (after processing),\n",
    "                                      #   \"all_pre_batches\": (other agent ids),\n",
    "                                      # }\n",
    "    },\n",
    "```\n",
    "in addition, it looks like there are additional 'events' given in the `build_trainer` function. for example, if we look at the [`dqn.py`](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/dqn.py) file:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "```\n",
    "we see that there are `before_train_step`, `after_optimizer_step` and `after_train_result`. what are these ? \n",
    "in [`trainer_template.py`](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer_template.py) we can find the documentation of these functions:\n",
    "- **after_init** (func) : optional function to run at the end of trainer init that takes the trainer instance as argument  \n",
    "- **before_train_step** (func): optional callback to run **before each train() call**. It takes the trainer instance as an argument.\n",
    "- **after_optimizer_step** (func): optional callback to run after each step() call to the policy optimizer. It takes the trainer instance and the policy gradient fetches as arguments.  \n",
    "- **after_train_result** (func): optional callback to run at the end of each train() call. It takes the trainer instance and result dict as arguments, and may mutate the result dict as needed.  \n",
    "- **collect_metrics_fn** (func): override the method used to collect metrics. It takes the trainer instance as argumnt.  \n",
    "- **before_evaluate_fn** (func): callback to run before evaluation. This takes the trainer instance as argument.\n",
    "\n",
    "all these callback functions are called from within the `_train` implementation of the `trainer_cls` that is built (see note below).\n",
    "\n",
    "> **Note** the callback function in the configuration all get `info` dict as an argument. the functions defined in the `build_trainer` take the trainer instance as an argument. we need to understand what is this `info` dict but it looks like it wont be enough to save a checkpoint. It looks like the more (or only) appropriate place to do it is in the `after_train_result`.\n",
    "\n",
    "\n",
    "> Note that the `build_trainer` function takes a `Trainer` class as a base class, add some `mixins` and define a subclass `trainer_cls` which is returned to the caller of `build_trainer`.  \n",
    "> Note that `trainer_cls` has implementation of both `__init__` and `_init`.  the `_init` is called from `_setup` of the parent class (`Trainer`). and the `Trainer._setup` is called from the `Trainer`'s parent class (`Trainable`) during its `__init__`. \n",
    "\n",
    "if we look at the documentation on [Contributing to Rllib](https://ray.readthedocs.io/en/latest/rllib-dev.html#contributing-to-rllib) we see that they describe how to create an agent:  \n",
    "\"*It takes just two changes to add an algorithm to contrib. A minimal example can be found here. First, subclass `Trainer` and implement the `_init` and `_train` methods*\"  \n",
    "and this is exactly what `build_trainer` does. so we'll use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the callbacks are related ?\n",
    "so we see we have 2 ways to define callbacks. what is the relation between them ?\n",
    "Note that the call to `tune.run` can be done without having access to the trainer definition. i.e. we can `pip install ray` and write a script that doesnt need to change the code of the trainer in order to define the callback.\n",
    "\n",
    "#### `on_train_result` vs `after_train_result`\n",
    "`on_train_result` is the callback that `tune.run` gets in the `config['callbacks']`. we can see that we can define this function without having access to the trainer definition code. we can use a registered trainer (\"PG\", \"DQN\", \"PPO\") and this will use the trainer registered with these strings. in the above example we used \"PG\", but we could have used \"DQN\" and then it would use `DQNTrainer` that is defined in the rllib `dqn.py`:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "\n",
    "DQNTrainer = GenericOffPolicyTrainer.with_updates(\n",
    "    name=\"DQN\", default_policy=DQNTFPolicy, default_config=DEFAULT_CONFIG)\n",
    "```\n",
    "\n",
    "Note that within this `dqn.py` we define the `DQNTrainer` with a function called `after_train_result`. which is described in the following\n",
    "\n",
    "\n",
    "`after_train_result` \n",
    "if we build a custom trainer using the `build_trainer`, we could also define the `after_train_result` callback by ourselves and then do whatever we need there. \n",
    "\n",
    "\n",
    "**So when each of these routines is called ?**\n",
    "\n",
    "Let's follow the `train()` method. \n",
    "as we described, the `build_trainer` returns a `trainer_cls` which is child of `Trainer` and implements the `_train()` method.\n",
    "when we provide tune with a trainer (e.g. \"DQN\") , it invokes its `train()` method. \n",
    "this `train()` is implemented in `Trainer` :\n",
    "\n",
    "```\n",
    "    @override(Trainable)\n",
    "    @PublicAPI\n",
    "    def train(self):\n",
    "        \"\"\"Overrides super.train to synchronize global vars.\"\"\"\n",
    "        ...\n",
    "        result = None\n",
    "        for _ in range(1 + MAX_WORKER_FAILURE_RETRIES):\n",
    "            try:\n",
    "                result = Trainable.train(self)\n",
    "            except RayError as e:\n",
    "                ...\n",
    "\n",
    "        if hasattr(self, \"workers\") and isinstance(self.workers, WorkerSet):\n",
    "            self._sync_filters_if_needed(self.workers)\n",
    "\n",
    "        ...\n",
    "        \n",
    "        if self.config[\"evaluation_interval\"] == 1 or (\n",
    "                self._iteration > 0 and self.config[\"evaluation_interval\"]\n",
    "                and self._iteration % self.config[\"evaluation_interval\"] == 0):\n",
    "            evaluation_metrics = self._evaluate()\n",
    "            assert isinstance(evaluation_metrics, dict), \\\n",
    "                \"_evaluate() needs to return a dict.\"\n",
    "            result.update(evaluation_metrics)\n",
    "\n",
    "        return result\n",
    "\n",
    "```\n",
    "so we see a call to the parent `Trainable.train()` method with the instance of the `Trainer`. let's look at this function:\n",
    "```\n",
    "    def train(self):\n",
    "        \"\"\"Runs one logical iteration of training.\n",
    "\n",
    "        Subclasses should override ``_train()`` instead to return results.\n",
    "        ...\n",
    "\n",
    "        Returns:\n",
    "            A dict that describes training progress.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        result = self._train()\n",
    "        assert isinstance(result, dict), \"_train() needs to return a dict.\"\n",
    "\n",
    "        # We do not modify internal state nor update this result if duplicate.\n",
    "        if RESULT_DUPLICATE in result:\n",
    "            return result\n",
    "\n",
    "        result = result.copy()\n",
    "\n",
    "        self._iteration += 1\n",
    "        self._iterations_since_restore += 1\n",
    "\n",
    "        if result.get(TIME_THIS_ITER_S) is not None:\n",
    "            time_this_iter = result[TIME_THIS_ITER_S]\n",
    "        else:\n",
    "            time_this_iter = time.time() - start\n",
    "        self._time_total += time_this_iter\n",
    "        self._time_since_restore += time_this_iter\n",
    "\n",
    "        result.setdefault(DONE, False)\n",
    "\n",
    "        # self._timesteps_total should only be tracked if increments provided\n",
    "        if result.get(TIMESTEPS_THIS_ITER) is not None:\n",
    "            if self._timesteps_total is None:\n",
    "                self._timesteps_total = 0\n",
    "            self._timesteps_total += result[TIMESTEPS_THIS_ITER]\n",
    "            self._timesteps_since_restore += result[TIMESTEPS_THIS_ITER]\n",
    "\n",
    "        # self._episodes_total should only be tracked if increments provided\n",
    "        if result.get(EPISODES_THIS_ITER) is not None:\n",
    "            if self._episodes_total is None:\n",
    "                self._episodes_total = 0\n",
    "            self._episodes_total += result[EPISODES_THIS_ITER]\n",
    "\n",
    "        # self._timesteps_total should not override user-provided total\n",
    "        result.setdefault(TIMESTEPS_TOTAL, self._timesteps_total)\n",
    "        result.setdefault(EPISODES_TOTAL, self._episodes_total)\n",
    "        result.setdefault(TRAINING_ITERATION, self._iteration)\n",
    "\n",
    "        # Provides auto-filled neg_mean_loss for avoiding regressions\n",
    "        if result.get(\"mean_loss\"):\n",
    "            result.setdefault(\"neg_mean_loss\", -result[\"mean_loss\"])\n",
    "\n",
    "        now = datetime.today()\n",
    "        result.update(\n",
    "            experiment_id=self._experiment_id,\n",
    "            date=now.strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "            timestamp=int(time.mktime(now.timetuple())),\n",
    "            time_this_iter_s=time_this_iter,\n",
    "            time_total_s=self._time_total,\n",
    "            pid=os.getpid(),\n",
    "            hostname=os.uname()[1],\n",
    "            node_ip=self._local_ip,\n",
    "            config=self.config,\n",
    "            time_since_restore=self._time_since_restore,\n",
    "            timesteps_since_restore=self._timesteps_since_restore,\n",
    "            iterations_since_restore=self._iterations_since_restore)\n",
    "\n",
    "        monitor_data = self._monitor.get_data()\n",
    "        if monitor_data:\n",
    "            result.update(monitor_data)\n",
    "\n",
    "        self._log_result(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "```\n",
    "\n",
    "There are 2 important things to note:\n",
    "1. at the beginning there is a call to `result = self._train()` that is implemented by the subclass\n",
    "2. at the end there is a call to `self._log_result(result)` \n",
    "\n",
    "\n",
    "first, let's look into `self._train()` that is implemented in the `trainer_cls` :\n",
    "```\n",
    "        def _train(self):\n",
    "            if self.train_exec_impl:\n",
    "                return self._train_exec_impl()\n",
    "\n",
    "            if before_train_step:\n",
    "                before_train_step(self)\n",
    "            prev_steps = self.optimizer.num_steps_sampled\n",
    "\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                fetches = self.optimizer.step()\n",
    "                if after_optimizer_step:\n",
    "                    after_optimizer_step(self, fetches)\n",
    "                if (time.time() - start >= self.config[\"min_iter_time_s\"]\n",
    "                        and self.optimizer.num_steps_sampled - prev_steps >=\n",
    "                        self.config[\"timesteps_per_iteration\"]):\n",
    "                    break\n",
    "\n",
    "            if collect_metrics_fn:\n",
    "                res = collect_metrics_fn(self)\n",
    "            else:\n",
    "                res = self.collect_metrics()\n",
    "            res.update(\n",
    "                timesteps_this_iter=self.optimizer.num_steps_sampled -\n",
    "                prev_steps,\n",
    "                info=res.get(\"info\", {}))\n",
    "\n",
    "            if after_train_result:\n",
    "                after_train_result(self, res)\n",
    "            return res\n",
    "\n",
    "```\n",
    "we see that within this method, we call the callbacks provided as argument to `build_trainer`. specifically, see at the bottom the call to `after_train_result(self,res)`. \n",
    "\n",
    "\n",
    "\n",
    "second, if we look at the line `self._log_result(result)` towards the end of `train()` implementation (in the `Trainer`). it calls the following `Trainer._log_result()`:\n",
    "```\n",
    "    @override(Trainable)\n",
    "    def _log_result(self, result):\n",
    "        if self.config[\"callbacks\"].get(\"on_train_result\"):\n",
    "            self.config[\"callbacks\"][\"on_train_result\"]({\n",
    "                \"trainer\": self,\n",
    "                \"result\": result,\n",
    "            })\n",
    "        # log after the callback is invoked, so that the user has a chance\n",
    "        # to mutate the result\n",
    "        Trainable._log_result(self, result)\n",
    "```\n",
    "we see that it calls the callback we have defined in the `config['Callbacks']` argument to `tune.run()`.\n",
    "\n",
    "in this manner we can track each of the callbacks provided to `tune.run` and see at which point they are called. \n",
    "\n",
    "**Bottom line**\n",
    "we should strive to define the model/checkpoint store using the callbacks provided to `tune.run()` s.t. we dont have to define a callback per each trainer (e.g. DQN, PG, PPO etc.). \n",
    "if we were defining it as input to `build_trainer` we should have implemented the callback in each of the trainers definition files (dqn.py, ppo.py etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while tracking the `save` method of the trainer, I noticed that it's base class (`Trainable`) has method `export_model`.\n",
    "This method calls an internal `_export_model` method that should be implemented by `Trainable` subclass. \n",
    "and it is indeed implemented in rllib's `Trainer` class.\n",
    "```\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        ExportFormat.validate(export_formats)\n",
    "        exported = {}\n",
    "        if ExportFormat.CHECKPOINT in export_formats:\n",
    "            path = os.path.join(export_dir, ExportFormat.CHECKPOINT)\n",
    "            self.export_policy_checkpoint(path)\n",
    "            exported[ExportFormat.CHECKPOINT] = path\n",
    "        if ExportFormat.MODEL in export_formats:\n",
    "            path = os.path.join(export_dir, ExportFormat.MODEL)\n",
    "            self.export_policy_model(path)\n",
    "            exported[ExportFormat.MODEL] = path\n",
    "        return exported\n",
    "```\n",
    "\n",
    "as we can see it calls `self.export_policy_checkpoint` and `self.export_policy_model`. \n",
    "I think that as the trainer has multiple workers, this is the place to call on each worker to save its policy instance. \n",
    "I guess that this function will use the policy API to save the checkpoint. \n",
    "\n",
    "the interaction between the `Trainer` object and the `policy` object is that the trainer manages the workers and each has a policy object to run with. \n",
    "see for example the implementation of `export_policy_checkpoint` in the Trainer: \n",
    "```\n",
    "    @DeveloperAPI\n",
    "    def export_policy_model(self, export_dir, policy_id=DEFAULT_POLICY_ID):\n",
    "        \"\"\"Export policy model with given policy_id to local directory.\n",
    "\n",
    "        self.workers.local_worker().export_policy_model(export_dir, policy_id)\n",
    "\n",
    "    @DeveloperAPI\n",
    "    def export_policy_checkpoint(self,\n",
    "                                 export_dir,\n",
    "                                 filename_prefix=\"model\",\n",
    "                                 policy_id=DEFAULT_POLICY_ID):\n",
    "        \"\"\"Export tensorflow policy model checkpoint to local directory.\n",
    "\n",
    "        self.workers.local_worker().export_policy_checkpoint(\n",
    "            export_dir, filename_prefix, policy_id)\n",
    "```\n",
    "\n",
    "Question: how does `export_policy_model` and `export_policy_checkpoint` is implemented ?\n",
    "we see that the trainer calls the `local_worker` method on its `workers` member (`WorkerSet` type). if we look at `WorkerSet` implmenetation ([worker_set.py](https://github.com/ray-project/ray/blob/master/rllib/evaluation/worker_set.py))we see that the `local_worker` is a `Rolloutworker` :\n",
    "```\n",
    "# in  WorkerSet.__init__:\n",
    "    # Always create a local worker\n",
    "    self._local_worker = self._make_worker(\n",
    "        RolloutWorker, env_creator, policy, 0, self._local_config)\n",
    "```\n",
    "\n",
    "and this `RolloutWorker` is implemented in [rollout_worker.py](https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py) and has a method that calls to its policy object to export the model:\n",
    "\n",
    "```\n",
    "    @DeveloperAPI\n",
    "    def export_policy_model(self, export_dir, policy_id=DEFAULT_POLICY_ID):\n",
    "        self.policy_map[policy_id].export_model(export_dir)\n",
    "\n",
    "```\n",
    "\n",
    "in a very similar way the checkpoint is saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rllib Policy object\n",
    "[`CLASS ray.rllib.policy.Policy(observation_space, action_space, config)`](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#module-ray.rllib.policy)  \n",
    "This object defines how to act in the environment, and also losses used to improve the policy based on its experiences. Note that both policy and loss are defined together for convenience, though the policy itself is logically separate.\n",
    "\n",
    "All policies can directly extend Policy, however TensorFlow users may find TFPolicy simpler to implement. TFPolicy also enables RLlib to apply TensorFlow-specific optimizations such as fusing multiple policy graphs and multi-GPU support.\n",
    "\n",
    "**TODO** need to check the relation between the policy object and the policy optimizer.\n",
    "the [Rllib policy](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#module-ray.rllib.policy) object has also an `export` method. when is it called ? \n",
    "\n",
    "**TODO** look at `TFPolicy` and `build_tf_policy` to understand whether I should directly save the policy model at the end and not a checkpoint of the trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How a policy object is created ?**\n",
    "when we build a trainer using `build_trainer` we provide it with `deafult_policy` which is the policy class that we want to train.  for example, in dqn:\n",
    "```\n",
    "GenericOffPolicyTrainer = build_trainer(\n",
    "    name=\"GenericOffPolicyAlgorithm\",\n",
    "    default_policy=None,\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    validate_config=validate_config_and_setup_param_noise,\n",
    "    get_initial_state=get_initial_state,\n",
    "    make_policy_optimizer=make_policy_optimizer,\n",
    "    before_train_step=update_worker_exploration,\n",
    "    after_optimizer_step=update_target_if_needed,\n",
    "    after_train_result=after_train_result,\n",
    "    execution_plan=execution_plan)\n",
    "\n",
    "DQNTrainer = GenericOffPolicyTrainer.with_updates(name=\"DQN\", default_policy=DQNTFPolicy, default_config=DEFAULT_CONFIG)\n",
    "```\n",
    "This policy class (e.g. `DQNTFPolicy`) can be built in 2 main ways, similar to `Trainer` :\n",
    "1. Directly inherit from the `Policy` class (or one of its descendants : `TFPolicy` &rarr; `DynamicTFPolic`)\n",
    "1. Use `build_tf_policy` to build the class. for example in dqn_policy.py:\n",
    "```\n",
    "DQNTFPolicy = build_tf_policy(\n",
    "    name=\"DQNTFPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.dqn.dqn.DEFAULT_CONFIG,\n",
    "    make_model=build_q_model,\n",
    "    action_sampler_fn=sample_action_from_q_network,\n",
    "    log_likelihood_fn=get_log_likelihood,\n",
    "    loss_fn=build_q_losses,\n",
    "    stats_fn=build_q_stats,\n",
    "    postprocess_fn=postprocess_nstep_and_prio,\n",
    "    optimizer_fn=adam_optimizer,\n",
    "    gradients_fn=clip_gradients,\n",
    "    extra_action_fetches_fn=lambda policy: {\"q_values\": policy.q_values},\n",
    "    extra_learn_fetches_fn=lambda policy: {\"td_error\": policy.q_loss.td_error},\n",
    "    before_init=setup_early_mixins,\n",
    "    before_loss_init=setup_mid_mixins,\n",
    "    after_init=setup_late_mixins,\n",
    "    obs_include_prev_action_reward=False,\n",
    "    mixins=[\n",
    "        ParameterNoiseMixin,\n",
    "        TargetNetworkMixin,\n",
    "        ComputeTDErrorMixin,\n",
    "        LearningRateSchedule,\n",
    "    ])\n",
    "```\n",
    "\n",
    "    this builder is implemented in [`tf_policy_template.py`](https://github.com/ray-project/ray/blob/master/rllib/policy/tf_policy_template.py) works very similar to `build_trainer`. it defines a base class that is constructed from `DynamicTFPolicy` and add some mixins that are provided as argument (see last argument in the above example) - and return this `policy_cls`.  \n",
    "\n",
    "All the mechanism of saving checkpoint or model is implemented in the `TFPolicy` (or the equivalent Pytorch class).\n",
    "\n",
    "What is the difference between saving a model and a checkpoint ? see [the implementation](https://ray.readthedocs.io/en/latest/_modules/ray/rllib/policy/tf_policy.html#TFPolicy.export_checkpoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming the flow\n",
    "OK, given the above, lets form the code for training while saving checkpoints\n",
    "\n",
    "it turns out that I can also let `tune.run` do the checkpoint management automatically (I searched for 'checkpoint' in the help):\n",
    "```\n",
    "    trials = tune.run(\n",
    "        \"PG\",\n",
    "        stop={\n",
    "            \"training_iteration\": args.num_iters,\n",
    "        },\n",
    "        config={\n",
    "            \"num_gpus\":1,\n",
    "            \"env\": \"CartPole-v0\",\n",
    "            \"callbacks\": {\n",
    "                \"on_episode_start\": on_episode_start,\n",
    "                \"on_episode_step\": on_episode_step,\n",
    "                \"on_episode_end\": on_episode_end,\n",
    "                \"on_sample_end\": on_sample_end,\n",
    "                \"on_train_result\": on_train_result,\n",
    "                \"on_postprocess_traj\": on_postprocess_traj,\n",
    "            },\n",
    "        },\n",
    "        checkpoint_at_end=True,\n",
    "        checkpoint_freq=50,\n",
    "        return_trials=True)\n",
    "\n",
    "```\n",
    "`checkpoint_at_end` asks to save checkpoint at the end of the run\n",
    "`checkpoint_freq` will instruct tune to save checkpoint every 50 iterations.\n",
    "\n",
    "we'll see the checkpoints in the results directory with `checkpoint_<iter>`.\n",
    "\n",
    "how do we load the checkpoint ? \n",
    "this is a checkpoint of the trainer. to use it for prediction, I followed the `rollout.py` code. see the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trial_results='/home/guy/ray_results/PG/PG_CartPole-v0_0_2020-04-01_17-56-43vl31kz32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guy/ray_results/PG/PG_CartPole-v0_0_2020-04-01_17-56-43vl31kz32\n"
     ]
    }
   ],
   "source": [
    "cd $path_to_trial_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mcheckpoint_100\u001b[00m\r\n",
      "│   ├── checkpoint-100\r\n",
      "│   └── checkpoint-100.tune_metadata\r\n",
      "├── \u001b[01;34mcheckpoint_110\u001b[00m\r\n",
      "│   ├── checkpoint-110\r\n",
      "│   └── checkpoint-110.tune_metadata\r\n",
      "├── \u001b[01;34mcheckpoint_50\u001b[00m\r\n",
      "│   ├── checkpoint-50\r\n",
      "│   └── checkpoint-50.tune_metadata\r\n",
      "├── events.out.tfevents.1585753003.guy-970\r\n",
      "├── params.json\r\n",
      "├── params.pkl\r\n",
      "├── \u001b[01;34mpolicy_model.h5\u001b[00m\r\n",
      "│   ├── events.out.tfevents.1585756992.guy-970\r\n",
      "│   ├── saved_model.pb\r\n",
      "│   └── \u001b[01;34mvariables\u001b[00m\r\n",
      "│       ├── variables.data-00000-of-00001\r\n",
      "│       └── variables.index\r\n",
      "├── progress.csv\r\n",
      "└── result.json\r\n",
      "\r\n",
      "5 directories, 15 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 07:55:53,998\tINFO resource_spec.py:212 -- Starting Ray with 13.62 GiB memory available for workers and up to 6.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-04-02 07:55:54,348\tINFO services.py:1120 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.93',\n",
       " 'redis_address': '192.168.1.93:55509',\n",
       " 'object_store_address': '/tmp/ray/session_2020-04-02_07-55-53_997504_23713/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-04-02_07-55-53_997504_23713/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-04-02_07-55-53_997504_23713'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import ray\n",
    "import pickle\n",
    "import numpy as np\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 07:55:59,297\tINFO trainer.py:427 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-04-02 07:55:59,322\tINFO trainer.py:584 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-04-02 07:56:01,712\tWARNING trainer_template.py:124 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "2020-04-02 07:56:01,713\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-04-02 07:56:01,714\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# we have to assume we know what agent was it...\n",
    "cls=get_agent_class(\"PG\")\n",
    "with open('params.pkl',\"rb\") as f:\n",
    "     config=pickle.load(f)\n",
    "agent=cls(env=config['env'],config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have the agent, we can restore a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 07:56:07,775\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-04-02 07:56:07,775\tINFO trainable.py:423 -- Restored on 192.168.1.93 from checkpoint: checkpoint_100/checkpoint-100\n",
      "2020-04-02 07:56:07,776\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 109.65056538581848, '_episodes_total': 548}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.rllib.policy.tf_policy_template.PGTFPolicy at 0x7fe5cc663050>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.restore('checkpoint_100/checkpoint-100')\n",
    "policy=agent.get_policy()\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 2)            514         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 67,843\n",
      "Trainable params: 67,843\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy.model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "e=gym.make(config['env'])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.1259754e+00, -2.3251294e+38, -4.0303564e-01, -8.3273986e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=e.reset()\n",
    "o=e.observation_space.sample()\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo=np.vstack([e.observation_space.sample() for _ in range(5)])\n",
    "oo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 1, 0]),\n",
       " [],\n",
       " {'action_prob': array([1., 1., 1., 1., 1.], dtype=float32),\n",
       "  'action_logp': array([0., 0., 0., 0., 0.], dtype=float32)})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.compute_actions(oo,explore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.export_model('policy_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 78344\r\n",
      "drwxrwxr-x 2 guy     4096 Apr  1 18:00 \u001b[0m\u001b[01;34mcheckpoint_100\u001b[0m/\r\n",
      "drwxrwxr-x 2 guy     4096 Apr  1 18:00 \u001b[01;34mcheckpoint_110\u001b[0m/\r\n",
      "drwxrwxr-x 2 guy     4096 Apr  1 17:58 \u001b[01;34mcheckpoint_50\u001b[0m/\r\n",
      "-rw-rw-r-- 1 guy   510442 Apr  1 18:00 events.out.tfevents.1585753003.guy-970\r\n",
      "-rw-rw-r-- 1 guy      494 Apr  1 17:56 params.json\r\n",
      "-rw-rw-r-- 1 guy     2223 Apr  1 17:56 params.pkl\r\n",
      "drwxr-xr-x 3 guy     4096 Apr  1 19:03 \u001b[01;34mpolicy_model.h5\u001b[0m/\r\n",
      "-rw-rw-r-- 1 guy 39618902 Apr  1 18:00 progress.csv\r\n",
      "-rw-rw-r-- 1 guy 40058562 Apr  1 18:00 result.json\r\n"
     ]
    }
   ],
   "source": [
    "ll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to see how to load the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 732\r\n",
      "-rw-rw-r-- 1 guy 356788 Apr  1 19:03 events.out.tfevents.1585756992.guy-970\r\n",
      "-rw-rw-r-- 1 guy 382025 Apr  1 19:03 saved_model.pb\r\n",
      "drwxr-xr-x 2 guy   4096 Apr  1 19:03 \u001b[0m\u001b[01;34mvariables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ll policy_model.h5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the export_model simply export a model for serving. not clear how it can be used like with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: policy_model.h5/saved_model.pb/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-160df527249e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'policy_model.h5/saved_model.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: policy_model.h5/saved_model.pb/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "m=tf.keras.models.load_model('policy_model.h5/saved_model.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TODO: figure out how to save the model s.t. it can be loaded as in the previous cell </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6089044ce62a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \"\"\"\n\u001b[1;32m    974\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m--> 975\u001b[0;31m                       signatures, options)\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3298\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3301\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1106\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "policy.model.base_model.save('base_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ccf4b0888a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3298\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3301\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20s/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1106\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "policy.model.base_model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fe5cc25fbd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.model.base_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
