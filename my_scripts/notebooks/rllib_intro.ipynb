{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rllib\n",
    "the purpose of this jupyter notebook is to have preliminary understanding of how Rllib is structured and working\n",
    "it is based on [their site](https://ray.readthedocs.io/en/latest/rllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of rllib stack](rllib-stack.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing ray\\[rllib\\] we can run it in either of 2 ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through shell command line\n",
    "! rllib train --run=PPO --env=CartPole-v0  # -v [-vv] for verbose,\n",
    "                                         # --eager [--trace] for eager execution,\n",
    "                                         # --torch to use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:59:50,614\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-08 21:59:50,616\tINFO resource_spec.py:212 -- Starting Ray with 13.62 GiB memory available for workers and up to 6.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-08 21:59:50,958\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.4/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:59:52,369\tWARNING worker.py:1058 -- The dashboard on node guy-970 failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 918, in <module>\n",
      "    redis_password=args.redis_password,\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 134, in __init__\n",
      "    self.setup_routes()\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 333, in setup_routes\n",
      "    build_dir)\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard (cd python/ray/dashboard/client && npm ci && npm run build): '/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/client/build'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:53,991\tINFO trainer.py:423 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:53,992\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:57,200\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 21.884615384615383\n",
      "  episode_reward_max: 56.0\n",
      "  episode_reward_mean: 21.884615384615383\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 182\n",
      "  episodes_total: 182\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2956.736\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6622737050056458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03214642405509949\n",
      "        policy_loss: -0.04698919504880905\n",
      "        total_loss: 66.82244873046875\n",
      "        vf_explained_var: 0.17032207548618317\n",
      "        vf_loss: 66.8630142211914\n",
      "    load_time_ms: 64.519\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 2295.349\n",
      "    update_time_ms: 593.019\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.32222222222223\n",
      "    ram_util_percent: 11.344444444444443\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.050136531846991544\n",
      "    mean_inference_ms: 0.7270158141449294\n",
      "    mean_processing_ms: 0.14662957084232545\n",
      "  time_since_restore: 5.960052251815796\n",
      "  time_this_iter_s: 5.960052251815796\n",
      "  time_total_s: 5.960052251815796\n",
      "  timestamp: 1583697603\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\"> 21.8846</td><td style=\"text-align: right;\">         5.96005</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">     1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 64.53\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 64.53\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 315\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2773.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5763375163078308\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009435108862817287\n",
      "        policy_loss: -0.016214197501540184\n",
      "        total_loss: 575.292724609375\n",
      "        vf_explained_var: 0.11430466920137405\n",
      "        vf_loss: 575.3060302734375\n",
      "    load_time_ms: 22.589\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 1939.68\n",
      "    update_time_ms: 200.727\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.18333333333334\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04921364680435799\n",
      "    mean_inference_ms: 0.7054635863583301\n",
      "    mean_processing_ms: 0.13442281575899873\n",
      "  time_since_restore: 14.866983890533447\n",
      "  time_this_iter_s: 4.445326089859009\n",
      "  time_total_s: 14.866983890533447\n",
      "  timestamp: 1583697612\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">   64.53</td><td style=\"text-align: right;\">          14.867</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 123.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 123.0\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2735.938\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5778759121894836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0026752587873488665\n",
      "        policy_loss: -0.0024575612042099237\n",
      "        total_loss: 412.6715393066406\n",
      "        vf_explained_var: 0.24595040082931519\n",
      "        vf_loss: 412.6732177734375\n",
      "    load_time_ms: 14.141\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 1859.423\n",
      "    update_time_ms: 122.239\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.18571428571428\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04922062843009021\n",
      "    mean_inference_ms: 0.7018611762418943\n",
      "    mean_processing_ms: 0.13063912105609532\n",
      "  time_since_restore: 23.724369525909424\n",
      "  time_this_iter_s: 4.4120752811431885\n",
      "  time_total_s: 23.724369525909424\n",
      "  timestamp: 1583697621\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">     123</td><td style=\"text-align: right;\">         23.7244</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">     5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 170.26\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 170.26\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 401\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2720.235\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.533046305179596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003753898199647665\n",
      "        policy_loss: -0.0016926942626014352\n",
      "        total_loss: 405.57562255859375\n",
      "        vf_explained_var: 0.38045212626457214\n",
      "        vf_loss: 405.5767822265625\n",
      "    load_time_ms: 10.512\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 1829.733\n",
      "    update_time_ms: 88.948\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.11666666666666\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049141814758244175\n",
      "    mean_inference_ms: 0.69864214671562\n",
      "    mean_processing_ms: 0.12689987467657698\n",
      "  time_since_restore: 32.61898756027222\n",
      "  time_this_iter_s: 4.440171241760254\n",
      "  time_total_s: 32.61898756027222\n",
      "  timestamp: 1583697630\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  170.26</td><td style=\"text-align: right;\">          32.619</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">     7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 194.57\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.57\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 443\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2711.507\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5317543745040894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006032226141542196\n",
      "        policy_loss: -0.003467428032308817\n",
      "        total_loss: 199.48553466796875\n",
      "        vf_explained_var: 0.7040902972221375\n",
      "        vf_loss: 199.48854064941406\n",
      "    load_time_ms: 8.564\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 1814.089\n",
      "    update_time_ms: 70.549\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.06666666666666\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04916175024095467\n",
      "    mean_inference_ms: 0.6966321486635673\n",
      "    mean_processing_ms: 0.1243542787357561\n",
      "  time_since_restore: 41.522605895996094\n",
      "  time_this_iter_s: 4.459575891494751\n",
      "  time_total_s: 41.522605895996094\n",
      "  timestamp: 1583697638\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  194.57</td><td style=\"text-align: right;\">         41.5226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">     9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 197.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.62\n",
      "  episode_reward_min: 163.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 483\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.956\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5371783375740051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0033627119846642017\n",
      "        policy_loss: -0.0021967533975839615\n",
      "        total_loss: 247.0288848876953\n",
      "        vf_explained_var: 0.5363666415214539\n",
      "        vf_loss: 247.03086853027344\n",
      "    load_time_ms: 1.541\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 43648\n",
      "    sample_time_ms: 1753.544\n",
      "    update_time_ms: 5.213\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.98571428571428\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049184298623196517\n",
      "    mean_inference_ms: 0.6960411261415723\n",
      "    mean_processing_ms: 0.12313685521535048\n",
      "  time_since_restore: 50.390357971191406\n",
      "  time_this_iter_s: 4.433572053909302\n",
      "  time_total_s: 50.390357971191406\n",
      "  timestamp: 1583697647\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  197.62</td><td style=\"text-align: right;\">         50.3904</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 195.07\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.07\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 524\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49065572023391724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008265562355518341\n",
      "        policy_loss: -0.004980773665010929\n",
      "        total_loss: 161.4665069580078\n",
      "        vf_explained_var: 0.7208117246627808\n",
      "        vf_loss: 161.47117614746094\n",
      "    load_time_ms: 1.465\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 1751.111\n",
      "    update_time_ms: 5.183\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.01666666666667\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914832194686155\n",
      "    mean_inference_ms: 0.6954656692919952\n",
      "    mean_processing_ms: 0.12227849085461541\n",
      "  time_since_restore: 59.268330335617065\n",
      "  time_this_iter_s: 4.444728374481201\n",
      "  time_total_s: 59.268330335617065\n",
      "  timestamp: 1583697656\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  195.07</td><td style=\"text-align: right;\">         59.2683</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 197.23\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.23\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 564\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2679.85\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4736994802951813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003971726167947054\n",
      "        policy_loss: -0.002383600687608123\n",
      "        total_loss: 138.18190002441406\n",
      "        vf_explained_var: 0.7769489288330078\n",
      "        vf_loss: 138.1842041015625\n",
      "    load_time_ms: 1.481\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 59520\n",
      "    sample_time_ms: 1756.336\n",
      "    update_time_ms: 5.29\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914393517007378\n",
      "    mean_inference_ms: 0.695096395893875\n",
      "    mean_processing_ms: 0.12173307470396154\n",
      "  time_since_restore: 68.1917986869812\n",
      "  time_this_iter_s: 4.470807790756226\n",
      "  time_total_s: 68.1917986869812\n",
      "  timestamp: 1583697665\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  197.23</td><td style=\"text-align: right;\">         68.1918</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 199.32\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.32\n",
      "  episode_reward_min: 171.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 604\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2680.291\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.004687500186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5036214590072632\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0053834011778235435\n",
      "        policy_loss: -0.003265487030148506\n",
      "        total_loss: 163.64669799804688\n",
      "        vf_explained_var: 0.7356401085853577\n",
      "        vf_loss: 163.64991760253906\n",
      "    load_time_ms: 1.522\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 67456\n",
      "    sample_time_ms: 1754.008\n",
      "    update_time_ms: 4.984\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.86666666666667\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049157775941731714\n",
      "    mean_inference_ms: 0.6949079443015594\n",
      "    mean_processing_ms: 0.12135368526972712\n",
      "  time_since_restore: 77.06475687026978\n",
      "  time_this_iter_s: 4.435238361358643\n",
      "  time_total_s: 77.06475687026978\n",
      "  timestamp: 1583697674\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.32</td><td style=\"text-align: right;\">         77.0648</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-23\n",
      "  done: false\n",
      "  episode_len_mean: 199.75\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.75\n",
      "  episode_reward_min: 183.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 644\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2679.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.004687500186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5162321925163269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004571177996695042\n",
      "        policy_loss: -0.0024070206563919783\n",
      "        total_loss: 217.0772247314453\n",
      "        vf_explained_var: 0.6417763233184814\n",
      "        vf_loss: 217.07958984375\n",
      "    load_time_ms: 1.485\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 75392\n",
      "    sample_time_ms: 1753.035\n",
      "    update_time_ms: 4.685\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.08571428571429\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914544994908546\n",
      "    mean_inference_ms: 0.6947284707599692\n",
      "    mean_processing_ms: 0.12103117992076942\n",
      "  time_since_restore: 85.94844222068787\n",
      "  time_this_iter_s: 4.436493635177612\n",
      "  time_total_s: 85.94844222068787\n",
      "  timestamp: 1583697683\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.75</td><td style=\"text-align: right;\">         85.9484</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">    19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 198.47\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.47\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 684\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2681.7\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5217893719673157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009161580353975296\n",
      "        policy_loss: -0.0025163141544908285\n",
      "        total_loss: 121.26493835449219\n",
      "        vf_explained_var: 0.8060898780822754\n",
      "        vf_loss: 121.26741790771484\n",
      "    load_time_ms: 1.505\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 83328\n",
      "    sample_time_ms: 1751.794\n",
      "    update_time_ms: 4.452\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.2\n",
      "    ram_util_percent: 11.583333333333334\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04912239364374445\n",
      "    mean_inference_ms: 0.6943734581528909\n",
      "    mean_processing_ms: 0.12074930231371042\n",
      "  time_since_restore: 94.82243800163269\n",
      "  time_this_iter_s: 4.414794921875\n",
      "  time_total_s: 94.82243800163269\n",
      "  timestamp: 1583697692\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.47</td><td style=\"text-align: right;\">         94.8224</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">    21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-41\n",
      "  done: false\n",
      "  episode_len_mean: 198.51\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.51\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 724\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2680.094\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4958638548851013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011287164874374866\n",
      "        policy_loss: -0.008030775003135204\n",
      "        total_loss: 253.19467163085938\n",
      "        vf_explained_var: 0.6218375563621521\n",
      "        vf_loss: 253.2026824951172\n",
      "    load_time_ms: 1.53\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 91264\n",
      "    sample_time_ms: 1751.875\n",
      "    update_time_ms: 4.577\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88333333333334\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04910582350701445\n",
      "    mean_inference_ms: 0.6941481560715986\n",
      "    mean_processing_ms: 0.12055291568235565\n",
      "  time_since_restore: 103.6867938041687\n",
      "  time_this_iter_s: 4.420777797698975\n",
      "  time_total_s: 103.6867938041687\n",
      "  timestamp: 1583697701\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.51</td><td style=\"text-align: right;\">         103.687</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">    23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 198.81\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.81\n",
      "  episode_reward_min: 171.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 765\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.365\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47822901606559753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0060133980587124825\n",
      "        policy_loss: -0.00287105911411345\n",
      "        total_loss: 251.18069458007812\n",
      "        vf_explained_var: 0.6215212941169739\n",
      "        vf_loss: 251.1835479736328\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 99200\n",
      "    sample_time_ms: 1749.988\n",
      "    update_time_ms: 4.519\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.75714285714285\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049082905996030754\n",
      "    mean_inference_ms: 0.6939192175005108\n",
      "    mean_processing_ms: 0.12036747719559926\n",
      "  time_since_restore: 112.57327246665955\n",
      "  time_this_iter_s: 4.43877911567688\n",
      "  time_total_s: 112.57327246665955\n",
      "  timestamp: 1583697710\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.81</td><td style=\"text-align: right;\">         112.573</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">    25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 199.16\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.16\n",
      "  episode_reward_min: 185.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 805\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2677.681\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46547481417655945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005870374385267496\n",
      "        policy_loss: -0.000582060485612601\n",
      "        total_loss: 138.92674255371094\n",
      "        vf_explained_var: 0.7581650614738464\n",
      "        vf_loss: 138.9273223876953\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 107136\n",
      "    sample_time_ms: 1752.359\n",
      "    update_time_ms: 4.795\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.97142857142856\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04907423650814658\n",
      "    mean_inference_ms: 0.693964038146042\n",
      "    mean_processing_ms: 0.12020259320467894\n",
      "  time_since_restore: 121.46614480018616\n",
      "  time_this_iter_s: 4.4242262840271\n",
      "  time_total_s: 121.46614480018616\n",
      "  timestamp: 1583697719\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.16</td><td style=\"text-align: right;\">         121.466</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">    27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 198.4\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.4\n",
      "  episode_reward_min: 176.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 846\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2676.752\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46278899908065796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004252588376402855\n",
      "        policy_loss: -0.00031634251354262233\n",
      "        total_loss: 197.55064392089844\n",
      "        vf_explained_var: 0.6675523519515991\n",
      "        vf_loss: 197.5509490966797\n",
      "    load_time_ms: 1.536\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 115072\n",
      "    sample_time_ms: 1747.086\n",
      "    update_time_ms: 4.786\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.25\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04904710685911159\n",
      "    mean_inference_ms: 0.6938274646987801\n",
      "    mean_processing_ms: 0.11996059552065379\n",
      "  time_since_restore: 130.2875108718872\n",
      "  time_this_iter_s: 4.406271696090698\n",
      "  time_total_s: 130.2875108718872\n",
      "  timestamp: 1583697728\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">   198.4</td><td style=\"text-align: right;\">         130.288</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">    29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4eb47ef26973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"env\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \"log_level\": \"INFO\" for verbose,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                      \u001b[0;31m# \"eager\": True for eager execution,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                      \u001b[0;31m# \"torch\": True for PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m         )\n\u001b[1;32m   1641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with python API (using tune)\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\"})  # \"log_level\": \"INFO\" for verbose,\n",
    "                                                     # \"eager\": True for eager execution,\n",
    "                                                     # \"torch\": True for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key concepts in Rllib\n",
    "There are 3 key concepts: Policies, Samples and Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "[Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies) are Python classes that define how an agent acts in an environment. [Rollout workers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. In a gym environment, there is a single agent and policy. In [vector envs](https://ray.readthedocs.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent](https://ray.readthedocs.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.  \n",
    "Policies can be implemented using any framework. However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions that let you define a trainable policy with a functional-style API, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_tf_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a7c7f2df2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m MyTFPolicy = build_tf_policy(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MyTFPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss_fn=policy_gradient_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_tf_policy' is not defined"
     ]
    }
   ],
   "source": [
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "Whether running in a single process or [large cluster](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources), all data interchange in RLlib is in the form of [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `sample_batch_size` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD.  \n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{ 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "  'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "  'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "  'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "  'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "  'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "  'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "  't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Policies each define a `learn_on_batch()` method that improves the policy given a sample batch of input. For TF and Torch policies, this is implemented using a loss function that takes as input sample batch tensors and outputs a scalar loss. Here are a few example loss functions:  \n",
    "- Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)\n",
    "- Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "- Importance-weighted [APPO surrogate loss](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_policy.py)  \n",
    "\n",
    "RLlib [Trainer classes](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers) coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging [policy optimizers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization) that implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of [these patterns](https://ray.readthedocs.io/en/latest/rllib-algorithms.html):  \n",
    "\n",
    "![a2c-arch](a2c-arch.svg)  \n",
    "\n",
    "\n",
    "RLlib uses [Ray actors](https://ray.readthedocs.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customization\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environment](https://ray.readthedocs.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://ray.readthedocs.io/en/latest/rllib-models.html#tensorflow-models), [action distribution](https://ray.readthedocs.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies):\n",
    "![rllib_components](rllib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conecpts and Custom Algorithms\n",
    "The following is selected items taken from the [documentation of rllib](https://ray.readthedocs.io/en/latest/rllib-toc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies)\n",
    "This section describes the internal concepts used to implement algorithms in RLlib. You might find this **useful if modifying or adding new algorithms to RLlib.**  \n",
    "Policy classes encapsulate the core numerical components of RL algorithms. typically includes:\n",
    " - Policy model that determines actions to take\n",
    " - A trajectory postprocessor for experiences\n",
    " - loss function to improve the policy given postprocessed experiences  \n",
    "\n",
    "for simple example, see the PG [policy definition](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)  \n",
    "\n",
    "Most of the interaction with deep learning is isolated to the [Policy interface](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py) allowing RLlib to support multiple frameworks. there are [Tensorflow](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [PyTorch](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) specific templates.  \n",
    "You can write your own from scratch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(Policy):\n",
    "    \"\"\"Example of a custom policy written from scratch.\n",
    "\n",
    "    You might find it more convenient to use the `build_tf_policy` and\n",
    "    `build_torch_policy` helpers instead for a real policy, which are\n",
    "    described in the next sections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        Policy.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return action batch, RNN states, extra values to include in batch\n",
    "        return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        # implement your learning code here\n",
    "        return {}  # return stats\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {\"w\": self.w}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.w = weights[\"w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using policy abstraction in multi agent, see the [rock-paper-scisors example](https://ray.readthedocs.io/en/latest/rllib-env.html#rock-paper-scissors-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building policies in Tensorflow\n",
    "describes how to build a tensorflow RLlib policy using `tf_policy_template.build_tf_policy()`  \n",
    "to start, we first have to define a loss function\n",
    "\n",
    "#### Define the loss function\n",
    "In RLlib, loss functions are defined over batches of trajectory data produced by policy evaluation. A basic policy gradient loss that only tries to maximize the 1-step reward can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    actions = train_batch[SampleBatch.ACTIONS]\n",
    "    rewards = train_batch[SampleBatch.REWARDS]\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(action_dist.logp(actions) * rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `actions` is a Tensor placeholder of shape \\[batch_size, action_dim...\\], and `rewards` is a placeholder of shape \\[batch_size\\].  \n",
    "<font color='red'> Question: why does the function gets policy as input ? I dont see it used anywhere </font>  \n",
    "The `action_dist` object is an [ActionDistribution](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#ray.rllib.models.ActionDistribution) that is parameterized by the output of the neural network policy model. Passing this loss function to `build_tf_policy` is enough to produce a very basic TF policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.tf_policy_template import build_tf_policy\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a trainer\n",
    "as an exercise (runnable file [here](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py)) we can create a [Trainer](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers)  and try running this policy on a toy env with two parallel rollout workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "MyTrainer = build_trainer(\n",
    "    name=\"MyCustomTrainer\",\n",
    "    default_policy=MyTFPolicy)\n",
    "\n",
    "ray.init()\n",
    "tune.run(MyTrainer, config={\"env\": \"CartPole-v0\", \"num_workers\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extending with postprocessing\n",
    "if we want to compute the advantage (sum of rewards over time) we need to define a trajectory postprocessor for the policy. this can be done by defining `postprocess_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.evaluation.postprocessing import compute_advantages, \\\n",
    "    Postprocessing\n",
    "\n",
    "def postprocess_advantages(policy,\n",
    "                           sample_batch,\n",
    "                           other_agent_batches=None,\n",
    "                           episode=None):\n",
    "    return compute_advantages(\n",
    "        sample_batch, 0.0, policy.config[\"gamma\"], use_gae=False, use_critic=False)\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[SampleBatch.ACTIONS]) *\n",
    "        train_batch[Postprocessing.ADVANTAGES])\n",
    "\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss,\n",
    "    postprocess_fn=postprocess_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how RLlib makes the advantages placeholder automatically available as `train_batch[Postprocessing.ADVANTAGES]` ?\n",
    "When building your policy, RLlib will create a “dummy” trajectory batch where all observations, actions, rewards, etc. are zeros. It then calls your postprocess_fn, and generates TF placeholders based on the numpy shapes of the postprocessed batch. RLlib tracks which placeholders that `loss_fn` and `stats_fn` access, and then feeds the corresponding sample data into those placeholders during loss optimization. You can also access these placeholders via `policy.get_placeholder(<name>)` after loss initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building policies in Eager\n",
    "Policies built with `build_tf_policy` (most of the reference algorithms are) can be run in eager mode by setting the `\"eager\": True` / `\"eager_tracing\": True` config options or using `rllib train --eager [--trace]`. This will tell RLlib to execute the model forward pass, action distribution, loss, and stats functions in eager mode.\n",
    "\n",
    "Eager mode makes debugging much easier, since you can now use line-by-line debugging with breakpoints or Python `print()` to inspect intermediate tensor values. However, eager can be slower than graph mode unless tracing is enabled.\n",
    "\n",
    "You can also selectively leverage eager operations within graph mode execution with tf.py_function. Here’s [an example](https://github.com/ray-project/ray/blob/master/rllib/examples/eager_execution.py) of using eager ops embedded within a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : PPO implementation\n",
    "in this example we'll see how the above flow is used to buid the PPO trainer and how we can modify it.  \n",
    "We'll go through the [PPO trainer definition](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo.py)\n",
    "\n",
    "\n",
    "at the bottom of the file, we are using `build_trainer` to build the PPOTrainer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PPOTrainer = build_trainer(\n",
    "    name=\"PPOTrainer\",    \n",
    "    default_policy=PPOTFPolicy,\n",
    "    \n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    make_policy_optimizer=choose_policy_optimizer,\n",
    "    validate_config=validate_config,\n",
    "    after_optimizer_step=update_kl,\n",
    "    before_train_step=warn_about_obs_filter,\n",
    "    after_train_result=warn_about_bad_reward_scales)\n",
    "```\n",
    "\n",
    "Lets dive into some of the parameters used above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### choose_policy_optimizer\n",
    "this function is fed through `make_policy_optimizer` and chooses which [Policy Optimizer](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization) to use for distributed training. You can think of these policy optimizers as coordinating the distributed workflow needed to improve the policy. Depending on the trainer config, PPO can switch between a simple synchronous optimizer, or a multi-GPU optimizer that implements minibatch SGD (the default):\n",
    "\n",
    "```\n",
    "def choose_policy_optimizer(workers, config):\n",
    "    if config[\"simple_optimizer\"]:\n",
    "        return SyncSamplesOptimizer(\n",
    "            workers,\n",
    "            num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "            train_batch_size=config[\"train_batch_size\"])\n",
    "\n",
    "    return LocalMultiGPUOptimizer(\n",
    "        workers,\n",
    "        sgd_batch_size=config[\"sgd_minibatch_size\"],\n",
    "        num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "        num_gpus=config[\"num_gpus\"],\n",
    "        sample_batch_size=config[\"sample_batch_size\"],\n",
    "        num_envs_per_worker=config[\"num_envs_per_worker\"],\n",
    "        train_batch_size=config[\"train_batch_size\"],\n",
    "        standardize_fields=[\"advantages\"],\n",
    "        straggler_mitigation=config[\"straggler_mitigation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to customize PPO to use an asynchronous-gradient optimization strategy similar to A3C. To do that, we could define a new function that returns `AsyncGradientsOptimizer` and override the `make_policy_optimizer` component of PPOTrainer:\n",
    "\n",
    "in the below code we'll see how to use `with_updates` to override specific fields of a predefined trainer.\n",
    "The `with_updates` method is also available for Torch and TF policies built from templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.optimizers import AsyncGradientsOptimizer\n",
    "\n",
    "def make_async_optimizer(workers, config):\n",
    "    return AsyncGradientsOptimizer(workers, grads_per_step=100)\n",
    "\n",
    "CustomTrainer = PPOTrainer.with_updates(\n",
    "    make_policy_optimizer=make_async_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update_kl\n",
    "This is used to adaptively adjust the KL penalty coefficient on the PPO loss, which bounds the policy change per training step. You’ll notice the code handles both single and multi-agent cases (where there are be multiple policies each with different KL coeffs):\n",
    "```\n",
    "def update_kl(trainer, fetches):\n",
    "    if \"kl\" in fetches:\n",
    "        # single-agent\n",
    "        trainer.workers.local_worker().for_policy(\n",
    "            lambda pi: pi.update_kl(fetches[\"kl\"]))\n",
    "    else:\n",
    "\n",
    "        def update(pi, pi_id):\n",
    "            if pi_id in fetches:\n",
    "                pi.update_kl(fetches[pi_id][\"kl\"])\n",
    "            else:\n",
    "                logger.debug(\"No data for {}, not updating kl\".format(pi_id))\n",
    "\n",
    "        # multi-agent\n",
    "        trainer.workers.local_worker().foreach_trainable_policy(update)\n",
    "```\n",
    "\n",
    "The `update_kl` method on the policy is defined in [PPOTFPolicy](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_policy.py) via the `KLCoeffMixin`, along with several other advanced features. Let’s look at each new feature used by the policy:\n",
    "```\n",
    "PPOTFPolicy = build_tf_policy(\n",
    "    name=\"PPOTFPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.ppo.ppo.DEFAULT_CONFIG,\n",
    "    loss_fn=ppo_surrogate_loss,\n",
    "    stats_fn=kl_and_loss_stats,\n",
    "    extra_action_fetches_fn=vf_preds_and_logits_fetches,\n",
    "    postprocess_fn=postprocess_ppo_gae,\n",
    "    gradients_fn=clip_gradients,\n",
    "    before_loss_init=setup_mixins,\n",
    "    mixins=[LearningRateSchedule, KLCoeffMixin, ValueNetworkMixin])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`stats_fn`**:  \n",
    "The stats function returns a dictionary of Tensors that will be reported with the training results. This also includes the `kl` metric which is used by the trainer to adjust the KL penalty. Note that many of the values below reference `policy.loss_obj`, which is assigned by `loss_fn` (not shown here since the PPO loss is quite complex). RLlib will always call `stats_fn` after `loss_fn`, so you can rely on using values saved by `loss_fn` as part of your statistics:\n",
    "\n",
    "```\n",
    "def kl_and_loss_stats(policy, train_batch):\n",
    "    policy.explained_variance = explained_variance(\n",
    "        train_batch[Postprocessing.VALUE_TARGETS], policy.model.value_function())\n",
    "\n",
    "    stats_fetches = {\n",
    "        \"cur_kl_coeff\": policy.kl_coeff,\n",
    "        \"cur_lr\": tf.cast(policy.cur_lr, tf.float64),\n",
    "        \"total_loss\": policy.loss_obj.loss,\n",
    "        \"policy_loss\": policy.loss_obj.mean_policy_loss,\n",
    "        \"vf_loss\": policy.loss_obj.mean_vf_loss,\n",
    "        \"vf_explained_var\": policy.explained_variance,\n",
    "        \"kl\": policy.loss_obj.mean_kl,\n",
    "        \"entropy\": policy.loss_obj.mean_entropy,\n",
    "    }\n",
    "\n",
    "    return stats_fetches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`extra_action_fetches_fn`**  \n",
    "This function defines extra outputs that will be recorded when generating actions with the policy.  \n",
    "For example, this enables saving the raw policy logits in the experience batch, which e.g. means it can be referenced in the PPO loss function via `batch[BEHAVIOUR_LOGITS]`. Other values such as the current value prediction can also be emitted for debugging or optimization purposes:\n",
    "```\n",
    "def vf_preds_and_logits_fetches(policy):\n",
    "    return {\n",
    "        SampleBatch.VF_PREDS: policy.model.value_function(),\n",
    "        BEHAVIOUR_LOGITS: policy.model.last_output(),\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`gradients_fn`**  \n",
    " If defined, this function returns TF gradients for the loss function. You’d typically only want to override this to apply transformations such as gradient clipping:  \n",
    "```\n",
    "def clip_gradients(policy, optimizer, loss):\n",
    "    if policy.config[\"grad_clip\"] is not None:\n",
    "        grads = tf.gradients(loss, policy.model.trainable_variables())\n",
    "        policy.grads, _ = tf.clip_by_global_norm(grads,\n",
    "                                                 policy.config[\"grad_clip\"])\n",
    "        clipped_grads = list(zip(policy.grads, policy.model.trainable_variables()))\n",
    "        return clipped_grads\n",
    "    else:\n",
    "        return optimizer.compute_gradients(\n",
    "            loss, colocate_gradients_with_ops=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`mixings`**  \n",
    "To add arbitrary stateful components, you can add mixin classes to the policy. Methods defined by these mixins will have higher priority than the base policy class, so you can use these to override methods (as in the case of `LearningRateSchedule`), or define extra methods and attributes (e.g., `KLCoeffMixin`, `ValueNetworkMixin`). Like any other Python superclass, these should be initialized at some point, which is what the `setup_mixins` function does:  \n",
    "```\n",
    "def setup_mixins(policy, obs_space, action_space, config):\n",
    "    ValueNetworkMixin.__init__(policy, obs_space, action_space, config)\n",
    "    KLCoeffMixin.__init__(policy, config)\n",
    "    LearningRateSchedule.__init__(policy, config[\"lr\"], config[\"lr_schedule\"])\n",
    "```\n",
    "In PPO we run `setup_mixins` before the loss function is called (i.e., `before_loss_init`), but other callbacks you can use include `before_init` and `after_init`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : DQN implementation\n",
    "Let’s look at how to implement a different family of policies, by looking at the [SimpleQ policy definition](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/simple_q_policy.py):  \n",
    "(Note that this is a simplified version of [DQNTFPolicy](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/dqn_policy.py))\n",
    "\n",
    "```\n",
    "SimpleQPolicy = build_tf_policy(\n",
    "    name=\"SimpleQPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.dqn.dqn.DEFAULT_CONFIG,\n",
    "    make_model=build_q_models,\n",
    "    action_sampler_fn=build_action_sampler,\n",
    "    loss_fn=build_q_losses,\n",
    "    extra_action_feed_fn=exploration_setting_inputs,\n",
    "    extra_action_fetches_fn=lambda policy: {\"q_values\": policy.q_values},\n",
    "    extra_learn_fetches_fn=lambda policy: {\"td_error\": policy.td_error},\n",
    "    before_init=setup_early_mixins,\n",
    "    after_init=setup_late_mixins,\n",
    "    obs_include_prev_action_reward=False,\n",
    "    mixins=[\n",
    "        ExplorationStateMixin,\n",
    "        TargetNetworkMixin,\n",
    "    ])\n",
    "```\n",
    "\n",
    "The biggest difference from the policy gradient policies you saw previously is that SimpleQPolicy defines its own `make_model` and `action_sampler_fn`. This means that the policy builder will not internally create a model and action distribution, rather it will call `build_q_models` and `build_action_sampler` to get the output action tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`build_q_models`**  \n",
    "The model creation function actually creates two different models for DQN: the base Q network, and also a target network. It requires each model to be of type `SimpleQModel`, which implements a `get_q_values()` method. The model catalog will raise an error if you try to use a custom ModelV2 model that isn’t a subclass of `SimpleQModel`. Similarly, the full DQN policy requires models to subclass `DistributionalQModel`, which implements `get_q_value_distributions()` and `get_state_value()`:  \n",
    "```\n",
    "def build_q_models(policy, obs_space, action_space, config):\n",
    "    ...\n",
    "\n",
    "    policy.q_model = ModelCatalog.get_model_v2(\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        config[\"model\"],\n",
    "        framework=\"tf\",\n",
    "        name=Q_SCOPE,\n",
    "        model_interface=SimpleQModel,\n",
    "        q_hiddens=config[\"hiddens\"])\n",
    "\n",
    "    policy.target_q_model = ModelCatalog.get_model_v2(\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        config[\"model\"],\n",
    "        framework=\"tf\",\n",
    "        name=Q_TARGET_SCOPE,\n",
    "        model_interface=SimpleQModel,\n",
    "        q_hiddens=config[\"hiddens\"])\n",
    "\n",
    "    return policy.q_model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`action_sampler`**\n",
    "The action sampler is straightforward, it just takes the q_model, runs a forward pass, and returns the argmax over the actions:\n",
    "```\n",
    "def build_action_sampler(policy, q_model, input_dict, obs_space, action_space,\n",
    "                         config):\n",
    "    # do max over Q values...\n",
    "    ...\n",
    "    return action, action_logp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of DQN is similar to other algorithms. Target updates are handled by a `after_optimizer_step` callback that periodically copies the weights of the Q network to the target.\n",
    "\n",
    "Finally, note that you do not have to use `build_tf_policy` to define a TensorFlow policy. You can alternatively subclass `Policy`, `TFPolicy`, or `DynamicTFPolicy` as convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending existing policies\n",
    "You can use the with_updates method on Trainers and Policy objects built with `make_*` to create a copy of the object with some changes, for example:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "\n",
    "CustomPolicy = PPOTFPolicy.with_updates(\n",
    "    name=\"MyCustomPPOTFPolicy\",\n",
    "    loss_fn=some_custom_loss_fn)\n",
    "\n",
    "CustomTrainer = PPOTrainer.with_updates(\n",
    "    default_policy=CustomPolicy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "Given an environment and policy, policy evaluation produces batches of experiences. This is your classic “environment interaction loop”. Efficient policy evaluation can be burdensome to get right, especially when leveraging vectorization, RNNs, or when operating in a multi-agent environment. RLlib provides a RolloutWorker class that manages all of this, and this class is used in most RLlib algorithms.\n",
    "\n",
    "You can use rollout workers standalone to produce batches of experiences. This can be done by calling `worker.sample()` on a worker instance, or `worker.sample.remote()` in parallel on worker instances created as Ray actors (see WorkerSet).\n",
    "\n",
    "Here is an example of creating a set of rollout workers and using them gather experiences in parallel. The trajectories are concatenated, the policy learns on the trajectory batch, and then we broadcast the policy weights to the workers for the next round of rollouts:\n",
    "```\n",
    "# Setup policy and rollout workers\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "workers = WorkerSet(\n",
    "    policy=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v0\"),\n",
    "    num_workers=10)\n",
    "\n",
    "while True:\n",
    "    # Gather a batch of samples\n",
    "    T1 = SampleBatch.concat_samples(\n",
    "        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n",
    "\n",
    "    # Improve the policy using the T1 batch\n",
    "    policy.learn_on_batch(T1)\n",
    "\n",
    "    # Broadcast weights to the policy evaluation workers\n",
    "    weights = ray.put({\"default_policy\": policy.get_weights()})\n",
    "    for w in workers.remote_workers():\n",
    "        w.set_weights.remote(weights)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization  \n",
    "Similar to how a [gradient-descent optimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) can be used to improve a model, RLlib’s [policy optimizers](https://github.com/ray-project/ray/tree/master/rllib/optimizers) implement different strategies for improving a policy.\n",
    "\n",
    "For example, in A3C you’d want to compute gradients asynchronously on different workers, and apply them to a central policy replica. This strategy is implemented by the [AsyncGradientsOptimizer](https://github.com/ray-project/ray/blob/master/rllib/optimizers/async_gradients_optimizer.py). Another alternative is to gather experiences synchronously in parallel and optimize the model centrally, as in [SyncSamplesOptimizer](https://github.com/ray-project/ray/blob/master/rllib/optimizers/sync_samples_optimizer.py). Policy optimizers abstract these strategies away into reusable modules.\n",
    "\n",
    "This is how the example in the previous section looks when written using a policy optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Same setup as before\n",
    "workers = WorkerSet(\n",
    "    policy=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v0\"),\n",
    "    num_workers=10)\n",
    "\n",
    "# this optimizer implements the IMPALA architecture\n",
    "optimizer = AsyncSamplesOptimizer(workers, train_batch_size=500)\n",
    "\n",
    "while True:\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers\n",
    "Trainers are the boilerplate classes that put the above components together, making algorithms accessible via Python API and the command line. They manage algorithm configuration, setup of the rollout workers and optimizer, and collection of training metrics. Trainers also implement the Trainable API for easy experiment management.\n",
    "\n",
    "Example of three equivalent ways of interacting with the PPO trainer, all of which log results in `~/ray_results`:\n",
    "\n",
    "```\n",
    "trainer = PPOTrainer(env=\"CartPole-v0\", config={\"train_batch_size\": 4000})\n",
    "while True:\n",
    "    print(trainer.train())\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "rllib train --run=PPO --env=CartPole-v0 --config='{\"train_batch_size\": 4000}'\n",
    "```\n",
    "\n",
    "```\n",
    "from ray import tune\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\", \"train_batch_size\": 4000})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlib20] *",
   "language": "python",
   "name": "conda-env-rlib20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.025px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
