{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rllib\n",
    "the purpose of this jupyter notebook is to have preliminary understanding of how Rllib is structured and working\n",
    "it is based on [their site](https://ray.readthedocs.io/en/latest/rllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of rllib stack](rllib-stack.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing ray\\[rllib\\] we can run it in either of 2 ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through shell command line\n",
    "! rllib train --run=PPO --env=CartPole-v0  # -v [-vv] for verbose,\n",
    "                                         # --eager [--trace] for eager execution,\n",
    "                                         # --torch to use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:59:50,614\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-08 21:59:50,616\tINFO resource_spec.py:212 -- Starting Ray with 13.62 GiB memory available for workers and up to 6.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-08 21:59:50,958\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.4/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:59:52,369\tWARNING worker.py:1058 -- The dashboard on node guy-970 failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 918, in <module>\n",
      "    redis_password=args.redis_password,\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 134, in __init__\n",
      "    self.setup_routes()\n",
      "  File \"/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 333, in setup_routes\n",
      "    build_dir)\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard (cd python/ray/dashboard/client && npm ci && npm run build): '/home/guy/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/dashboard/client/build'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:53,991\tINFO trainer.py:423 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:53,992\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=12071)\u001b[0m 2020-03-08 21:59:57,200\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 21.884615384615383\n",
      "  episode_reward_max: 56.0\n",
      "  episode_reward_mean: 21.884615384615383\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 182\n",
      "  episodes_total: 182\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2956.736\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6622737050056458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03214642405509949\n",
      "        policy_loss: -0.04698919504880905\n",
      "        total_loss: 66.82244873046875\n",
      "        vf_explained_var: 0.17032207548618317\n",
      "        vf_loss: 66.8630142211914\n",
      "    load_time_ms: 64.519\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 2295.349\n",
      "    update_time_ms: 593.019\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.32222222222223\n",
      "    ram_util_percent: 11.344444444444443\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.050136531846991544\n",
      "    mean_inference_ms: 0.7270158141449294\n",
      "    mean_processing_ms: 0.14662957084232545\n",
      "  time_since_restore: 5.960052251815796\n",
      "  time_this_iter_s: 5.960052251815796\n",
      "  time_total_s: 5.960052251815796\n",
      "  timestamp: 1583697603\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\"> 21.8846</td><td style=\"text-align: right;\">         5.96005</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">     1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 64.53\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 64.53\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 315\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2773.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5763375163078308\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009435108862817287\n",
      "        policy_loss: -0.016214197501540184\n",
      "        total_loss: 575.292724609375\n",
      "        vf_explained_var: 0.11430466920137405\n",
      "        vf_loss: 575.3060302734375\n",
      "    load_time_ms: 22.589\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 1939.68\n",
      "    update_time_ms: 200.727\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.18333333333334\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04921364680435799\n",
      "    mean_inference_ms: 0.7054635863583301\n",
      "    mean_processing_ms: 0.13442281575899873\n",
      "  time_since_restore: 14.866983890533447\n",
      "  time_this_iter_s: 4.445326089859009\n",
      "  time_total_s: 14.866983890533447\n",
      "  timestamp: 1583697612\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">   64.53</td><td style=\"text-align: right;\">          14.867</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 123.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 123.0\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2735.938\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5778759121894836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0026752587873488665\n",
      "        policy_loss: -0.0024575612042099237\n",
      "        total_loss: 412.6715393066406\n",
      "        vf_explained_var: 0.24595040082931519\n",
      "        vf_loss: 412.6732177734375\n",
      "    load_time_ms: 14.141\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 1859.423\n",
      "    update_time_ms: 122.239\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.18571428571428\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04922062843009021\n",
      "    mean_inference_ms: 0.7018611762418943\n",
      "    mean_processing_ms: 0.13063912105609532\n",
      "  time_since_restore: 23.724369525909424\n",
      "  time_this_iter_s: 4.4120752811431885\n",
      "  time_total_s: 23.724369525909424\n",
      "  timestamp: 1583697621\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">     123</td><td style=\"text-align: right;\">         23.7244</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">     5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 170.26\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 170.26\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 401\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2720.235\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.533046305179596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003753898199647665\n",
      "        policy_loss: -0.0016926942626014352\n",
      "        total_loss: 405.57562255859375\n",
      "        vf_explained_var: 0.38045212626457214\n",
      "        vf_loss: 405.5767822265625\n",
      "    load_time_ms: 10.512\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 1829.733\n",
      "    update_time_ms: 88.948\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.11666666666666\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049141814758244175\n",
      "    mean_inference_ms: 0.69864214671562\n",
      "    mean_processing_ms: 0.12689987467657698\n",
      "  time_since_restore: 32.61898756027222\n",
      "  time_this_iter_s: 4.440171241760254\n",
      "  time_total_s: 32.61898756027222\n",
      "  timestamp: 1583697630\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  170.26</td><td style=\"text-align: right;\">          32.619</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">     7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 194.57\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.57\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 443\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2711.507\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5317543745040894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006032226141542196\n",
      "        policy_loss: -0.003467428032308817\n",
      "        total_loss: 199.48553466796875\n",
      "        vf_explained_var: 0.7040902972221375\n",
      "        vf_loss: 199.48854064941406\n",
      "    load_time_ms: 8.564\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 1814.089\n",
      "    update_time_ms: 70.549\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.06666666666666\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04916175024095467\n",
      "    mean_inference_ms: 0.6966321486635673\n",
      "    mean_processing_ms: 0.1243542787357561\n",
      "  time_since_restore: 41.522605895996094\n",
      "  time_this_iter_s: 4.459575891494751\n",
      "  time_total_s: 41.522605895996094\n",
      "  timestamp: 1583697638\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  194.57</td><td style=\"text-align: right;\">         41.5226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">     9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 197.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.62\n",
      "  episode_reward_min: 163.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 483\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.956\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5371783375740051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0033627119846642017\n",
      "        policy_loss: -0.0021967533975839615\n",
      "        total_loss: 247.0288848876953\n",
      "        vf_explained_var: 0.5363666415214539\n",
      "        vf_loss: 247.03086853027344\n",
      "    load_time_ms: 1.541\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 43648\n",
      "    sample_time_ms: 1753.544\n",
      "    update_time_ms: 5.213\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.98571428571428\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049184298623196517\n",
      "    mean_inference_ms: 0.6960411261415723\n",
      "    mean_processing_ms: 0.12313685521535048\n",
      "  time_since_restore: 50.390357971191406\n",
      "  time_this_iter_s: 4.433572053909302\n",
      "  time_total_s: 50.390357971191406\n",
      "  timestamp: 1583697647\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  197.62</td><td style=\"text-align: right;\">         50.3904</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 195.07\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.07\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 524\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49065572023391724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008265562355518341\n",
      "        policy_loss: -0.004980773665010929\n",
      "        total_loss: 161.4665069580078\n",
      "        vf_explained_var: 0.7208117246627808\n",
      "        vf_loss: 161.47117614746094\n",
      "    load_time_ms: 1.465\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 1751.111\n",
      "    update_time_ms: 5.183\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.01666666666667\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914832194686155\n",
      "    mean_inference_ms: 0.6954656692919952\n",
      "    mean_processing_ms: 0.12227849085461541\n",
      "  time_since_restore: 59.268330335617065\n",
      "  time_this_iter_s: 4.444728374481201\n",
      "  time_total_s: 59.268330335617065\n",
      "  timestamp: 1583697656\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  195.07</td><td style=\"text-align: right;\">         59.2683</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">    13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 197.23\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.23\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 564\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2679.85\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.01875000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4736994802951813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003971726167947054\n",
      "        policy_loss: -0.002383600687608123\n",
      "        total_loss: 138.18190002441406\n",
      "        vf_explained_var: 0.7769489288330078\n",
      "        vf_loss: 138.1842041015625\n",
      "    load_time_ms: 1.481\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 59520\n",
      "    sample_time_ms: 1756.336\n",
      "    update_time_ms: 5.29\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914393517007378\n",
      "    mean_inference_ms: 0.695096395893875\n",
      "    mean_processing_ms: 0.12173307470396154\n",
      "  time_since_restore: 68.1917986869812\n",
      "  time_this_iter_s: 4.470807790756226\n",
      "  time_total_s: 68.1917986869812\n",
      "  timestamp: 1583697665\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  197.23</td><td style=\"text-align: right;\">         68.1918</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 199.32\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.32\n",
      "  episode_reward_min: 171.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 604\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2680.291\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.004687500186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5036214590072632\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0053834011778235435\n",
      "        policy_loss: -0.003265487030148506\n",
      "        total_loss: 163.64669799804688\n",
      "        vf_explained_var: 0.7356401085853577\n",
      "        vf_loss: 163.64991760253906\n",
      "    load_time_ms: 1.522\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 67456\n",
      "    sample_time_ms: 1754.008\n",
      "    update_time_ms: 4.984\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.86666666666667\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049157775941731714\n",
      "    mean_inference_ms: 0.6949079443015594\n",
      "    mean_processing_ms: 0.12135368526972712\n",
      "  time_since_restore: 77.06475687026978\n",
      "  time_this_iter_s: 4.435238361358643\n",
      "  time_total_s: 77.06475687026978\n",
      "  timestamp: 1583697674\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.32</td><td style=\"text-align: right;\">         77.0648</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">    17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-23\n",
      "  done: false\n",
      "  episode_len_mean: 199.75\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.75\n",
      "  episode_reward_min: 183.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 644\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2679.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.004687500186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5162321925163269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004571177996695042\n",
      "        policy_loss: -0.0024070206563919783\n",
      "        total_loss: 217.0772247314453\n",
      "        vf_explained_var: 0.6417763233184814\n",
      "        vf_loss: 217.07958984375\n",
      "    load_time_ms: 1.485\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 75392\n",
      "    sample_time_ms: 1753.035\n",
      "    update_time_ms: 4.685\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.08571428571429\n",
      "    ram_util_percent: 11.5\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04914544994908546\n",
      "    mean_inference_ms: 0.6947284707599692\n",
      "    mean_processing_ms: 0.12103117992076942\n",
      "  time_since_restore: 85.94844222068787\n",
      "  time_this_iter_s: 4.436493635177612\n",
      "  time_total_s: 85.94844222068787\n",
      "  timestamp: 1583697683\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.75</td><td style=\"text-align: right;\">         85.9484</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">    19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 198.47\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.47\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 684\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2681.7\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5217893719673157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009161580353975296\n",
      "        policy_loss: -0.0025163141544908285\n",
      "        total_loss: 121.26493835449219\n",
      "        vf_explained_var: 0.8060898780822754\n",
      "        vf_loss: 121.26741790771484\n",
      "    load_time_ms: 1.505\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 83328\n",
      "    sample_time_ms: 1751.794\n",
      "    update_time_ms: 4.452\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.2\n",
      "    ram_util_percent: 11.583333333333334\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04912239364374445\n",
      "    mean_inference_ms: 0.6943734581528909\n",
      "    mean_processing_ms: 0.12074930231371042\n",
      "  time_since_restore: 94.82243800163269\n",
      "  time_this_iter_s: 4.414794921875\n",
      "  time_total_s: 94.82243800163269\n",
      "  timestamp: 1583697692\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.47</td><td style=\"text-align: right;\">         94.8224</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">    21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-41\n",
      "  done: false\n",
      "  episode_len_mean: 198.51\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.51\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 724\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2680.094\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4958638548851013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011287164874374866\n",
      "        policy_loss: -0.008030775003135204\n",
      "        total_loss: 253.19467163085938\n",
      "        vf_explained_var: 0.6218375563621521\n",
      "        vf_loss: 253.2026824951172\n",
      "    load_time_ms: 1.53\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 91264\n",
      "    sample_time_ms: 1751.875\n",
      "    update_time_ms: 4.577\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88333333333334\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04910582350701445\n",
      "    mean_inference_ms: 0.6941481560715986\n",
      "    mean_processing_ms: 0.12055291568235565\n",
      "  time_since_restore: 103.6867938041687\n",
      "  time_this_iter_s: 4.420777797698975\n",
      "  time_total_s: 103.6867938041687\n",
      "  timestamp: 1583697701\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.51</td><td style=\"text-align: right;\">         103.687</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">    23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 198.81\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.81\n",
      "  episode_reward_min: 171.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 765\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2678.365\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47822901606559753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0060133980587124825\n",
      "        policy_loss: -0.00287105911411345\n",
      "        total_loss: 251.18069458007812\n",
      "        vf_explained_var: 0.6215212941169739\n",
      "        vf_loss: 251.1835479736328\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 99200\n",
      "    sample_time_ms: 1749.988\n",
      "    update_time_ms: 4.519\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.75714285714285\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.049082905996030754\n",
      "    mean_inference_ms: 0.6939192175005108\n",
      "    mean_processing_ms: 0.12036747719559926\n",
      "  time_since_restore: 112.57327246665955\n",
      "  time_this_iter_s: 4.43877911567688\n",
      "  time_total_s: 112.57327246665955\n",
      "  timestamp: 1583697710\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  198.81</td><td style=\"text-align: right;\">         112.573</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">    25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 199.16\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.16\n",
      "  episode_reward_min: 185.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 805\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2677.681\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46547481417655945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005870374385267496\n",
      "        policy_loss: -0.000582060485612601\n",
      "        total_loss: 138.92674255371094\n",
      "        vf_explained_var: 0.7581650614738464\n",
      "        vf_loss: 138.9273223876953\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 107136\n",
      "    sample_time_ms: 1752.359\n",
      "    update_time_ms: 4.795\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.97142857142856\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04907423650814658\n",
      "    mean_inference_ms: 0.693964038146042\n",
      "    mean_processing_ms: 0.12020259320467894\n",
      "  time_since_restore: 121.46614480018616\n",
      "  time_this_iter_s: 4.4242262840271\n",
      "  time_total_s: 121.46614480018616\n",
      "  timestamp: 1583697719\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">  199.16</td><td style=\"text-align: right;\">         121.466</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">    27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-08_22-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 198.4\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.4\n",
      "  episode_reward_min: 176.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 846\n",
      "  experiment_id: 3ed2ab9e0aea41c1ad8d62cf7e422c0b\n",
      "  experiment_tag: '0'\n",
      "  hostname: guy-970\n",
      "  info:\n",
      "    grad_time_ms: 2676.752\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0023437500931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46278899908065796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004252588376402855\n",
      "        policy_loss: -0.00031634251354262233\n",
      "        total_loss: 197.55064392089844\n",
      "        vf_explained_var: 0.6675523519515991\n",
      "        vf_loss: 197.5509490966797\n",
      "    load_time_ms: 1.536\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 115072\n",
      "    sample_time_ms: 1747.086\n",
      "    update_time_ms: 4.786\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.93\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.25\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 12071\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.04904710685911159\n",
      "    mean_inference_ms: 0.6938274646987801\n",
      "    mean_processing_ms: 0.11996059552065379\n",
      "  time_since_restore: 130.2875108718872\n",
      "  time_this_iter_s: 4.406271696090698\n",
      "  time_total_s: 130.2875108718872\n",
      "  timestamp: 1583697728\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.7/23.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/4 CPUs, 0/1 GPUs, 0.0/13.62 GiB heap, 0.0/4.69 GiB objects<br>Result logdir: /home/guy/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_00000</td><td>RUNNING </td><td>192.168.1.93:12071</td><td style=\"text-align: right;\">   198.4</td><td style=\"text-align: right;\">         130.288</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">    29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4eb47ef26973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"env\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \"log_level\": \"INFO\" for verbose,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                      \u001b[0;31m# \"eager\": True for eager execution,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                      \u001b[0;31m# \"torch\": True for PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlib20/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m         )\n\u001b[1;32m   1641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with python API (using tune)\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\"})  # \"log_level\": \"INFO\" for verbose,\n",
    "                                                     # \"eager\": True for eager execution,\n",
    "                                                     # \"torch\": True for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key concepts in Rllib\n",
    "There are 3 key concepts: Policies, Samples and Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "[Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies) are Python classes that define how an agent acts in an environment. [Rollout workers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. In a gym environment, there is a single agent and policy. In [vector envs](https://ray.readthedocs.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent](https://ray.readthedocs.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.  \n",
    "Policies can be implemented using any framework. However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions that let you define a trainable policy with a functional-style API, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_tf_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a7c7f2df2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m MyTFPolicy = build_tf_policy(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MyTFPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss_fn=policy_gradient_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_tf_policy' is not defined"
     ]
    }
   ],
   "source": [
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "Whether running in a single process or [large cluster](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources), all data interchange in RLlib is in the form of [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `sample_batch_size` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD.  \n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{ 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "  'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "  'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "  'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "  'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "  'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "  'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "  't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Policies each define a `learn_on_batch()` method that improves the policy given a sample batch of input. For TF and Torch policies, this is implemented using a loss function that takes as input sample batch tensors and outputs a scalar loss. Here are a few example loss functions:  \n",
    "- Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)\n",
    "- Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "- Importance-weighted [APPO surrogate loss](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_policy.py)  \n",
    "\n",
    "RLlib [Trainer classes](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers) coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging [policy optimizers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization) that implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of [these patterns](https://ray.readthedocs.io/en/latest/rllib-algorithms.html):  \n",
    "\n",
    "![a2c-arch](a2c-arch.svg)  \n",
    "\n",
    "\n",
    "RLlib uses [Ray actors](https://ray.readthedocs.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customization\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environment](https://ray.readthedocs.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://ray.readthedocs.io/en/latest/rllib-models.html#tensorflow-models), [action distribution](https://ray.readthedocs.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies):\n",
    "![rllib_components](rllib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conecpts and Custom Algorithms\n",
    "The following is selected items taken from the [documentation of rllib](https://ray.readthedocs.io/en/latest/rllib-toc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies)\n",
    "This section describes the internal concepts used to implement algorithms in RLlib. You might find this **useful if modifying or adding new algorithms to RLlib.**  \n",
    "Policy classes encapsulate the core numerical components of RL algorithms. typically includes:\n",
    " - Policy model that determines actions to take\n",
    " - A trajectory postprocessor for experiences\n",
    " - loss function to improve the policy given postprocessed experiences  \n",
    "\n",
    "for simple example, see the PG [policy definition](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)  \n",
    "\n",
    "Most of the interaction with deep learning is isolated to the [Policy interface](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py) allowing RLlib to support multiple frameworks. there are [Tensorflow](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [PyTorch](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) specific templates.  \n",
    "You can write your own from scratch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(Policy):\n",
    "    \"\"\"Example of a custom policy written from scratch.\n",
    "\n",
    "    You might find it more convenient to use the `build_tf_policy` and\n",
    "    `build_torch_policy` helpers instead for a real policy, which are\n",
    "    described in the next sections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        Policy.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return action batch, RNN states, extra values to include in batch\n",
    "        return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        # implement your learning code here\n",
    "        return {}  # return stats\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {\"w\": self.w}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.w = weights[\"w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using policy abstraction in multi agent, see the [rock-paper-scisors example](https://ray.readthedocs.io/en/latest/rllib-env.html#rock-paper-scissors-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building policies in Tensorflow\n",
    "describes how to build a tensorflow RLlib policy using `tf_policy_template.build_tf_policy()`\n",
    "\n",
    "#### Define the loss function\n",
    "In RLlib, loss functions are defined over batches of trajectory data produced by policy evaluation. A basic policy gradient loss that only tries to maximize the 1-step reward can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    actions = train_batch[SampleBatch.ACTIONS]\n",
    "    rewards = train_batch[SampleBatch.REWARDS]\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(action_dist.logp(actions) * rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `actions` is a Tensor placeholder of shape \\[batch_size, action_dim...\\], and `rewards` is a placeholder of shape \\[batch_size\\].  \n",
    "The `action_dist` object is an [ActionDistribution](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#ray.rllib.models.ActionDistribution) that is parameterized by the output of the neural network policy model. Passing this loss function to `build_tf_policy` is enough to produce a very basic TF policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.tf_policy_template import build_tf_policy\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as an exercise (runnable file [here](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py)) we can create a [Trainer](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers)  and try running this policy on a toy env with two parallel rollout workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "MyTrainer = build_trainer(\n",
    "    name=\"MyCustomTrainer\",\n",
    "    default_policy=MyTFPolicy)\n",
    "\n",
    "ray.init()\n",
    "tune.run(MyTrainer, config={\"env\": \"CartPole-v0\", \"num_workers\": 2})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlib20]",
   "language": "python",
   "name": "conda-env-rlib20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
