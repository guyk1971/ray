{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rllib\n",
    "the purpose of this jupyter notebook is to have preliminary understanding of how Rllib is structured and working\n",
    "it is based on [their site](https://ray.readthedocs.io/en/latest/rllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of rllib stack](rllib-stack.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing ray\\[rllib\\] we can run it in either of 2 ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through shell command line\n",
    "! rllib train --run=PPO --env=CartPole-v0  # -v [-vv] for verbose,\n",
    "                                         # --eager [--trace] for eager execution,\n",
    "                                         # --torch to use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with python API (using tune)\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\"})  # \"log_level\": \"INFO\" for verbose,\n",
    "                                                     # \"eager\": True for eager execution,\n",
    "                                                     # \"torch\": True for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key concepts in Rllib\n",
    "There are 3 key concepts: Policies, Samples and Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "[Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies) are Python classes that define how an agent acts in an environment. [Rollout workers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. In a gym environment, there is a single agent and policy. In [vector envs](https://ray.readthedocs.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent](https://ray.readthedocs.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.  \n",
    "Policies can be implemented using any framework. However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions that let you define a trainable policy with a functional-style API, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "Whether running in a single process or [large cluster](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources), all data interchange in RLlib is in the form of [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `sample_batch_size` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD.  \n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{ 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "  'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "  'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "  'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "  'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "  'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "  'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "  't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Policies each define a `learn_on_batch()` method that improves the policy given a sample batch of input. For TF and Torch policies, this is implemented using a loss function that takes as input sample batch tensors and outputs a scalar loss. Here are a few example loss functions:  \n",
    "- Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)\n",
    "- Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "- Importance-weighted [APPO surrogate loss](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_policy.py)  \n",
    "\n",
    "RLlib [Trainer classes](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers) coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging [policy optimizers](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization) that implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of [these patterns](https://ray.readthedocs.io/en/latest/rllib-algorithms.html):  \n",
    "\n",
    "![a2c-arch](a2c-arch.svg)  \n",
    "\n",
    "\n",
    "RLlib uses [Ray actors](https://ray.readthedocs.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customization\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environment](https://ray.readthedocs.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://ray.readthedocs.io/en/latest/rllib-models.html#tensorflow-models), [action distribution](https://ray.readthedocs.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies):\n",
    "![rllib_components](rllib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conecpts and Custom Algorithms\n",
    "The following is selected items taken from the [documentation of rllib](https://ray.readthedocs.io/en/latest/rllib-toc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policies](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policies)\n",
    "This section describes the internal concepts used to implement algorithms in RLlib. You might find this **useful if modifying or adding new algorithms to RLlib.**  \n",
    "Policy classes encapsulate the core numerical components of RL algorithms. typically includes:\n",
    " - Policy model that determines actions to take\n",
    " - A trajectory postprocessor for experiences\n",
    " - loss function to improve the policy given postprocessed experiences  \n",
    "\n",
    "for simple example, see the PG [policy definition](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py)  \n",
    "\n",
    "Most of the interaction with deep learning is isolated to the [Policy interface](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py) allowing RLlib to support multiple frameworks. there are [Tensorflow](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [PyTorch](https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) specific templates.  \n",
    "You can write your own from scratch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(Policy):\n",
    "    \"\"\"Example of a custom policy written from scratch.\n",
    "\n",
    "    You might find it more convenient to use the `build_tf_policy` and\n",
    "    `build_torch_policy` helpers instead for a real policy, which are\n",
    "    described in the next sections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        Policy.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return action batch, RNN states, extra values to include in batch\n",
    "        return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        # implement your learning code here\n",
    "        return {}  # return stats\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {\"w\": self.w}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.w = weights[\"w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using policy abstraction in multi agent, see the [rock-paper-scisors example](https://ray.readthedocs.io/en/latest/rllib-env.html#rock-paper-scissors-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building policies in Tensorflow\n",
    "describes how to build a tensorflow RLlib policy using `tf_policy_template.build_tf_policy()`  \n",
    "to start, we first have to define a loss function\n",
    "\n",
    "#### Define the loss function\n",
    "In RLlib, loss functions are defined over batches of trajectory data produced by policy evaluation. A basic policy gradient loss that only tries to maximize the 1-step reward can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    actions = train_batch[SampleBatch.ACTIONS]\n",
    "    rewards = train_batch[SampleBatch.REWARDS]\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(action_dist.logp(actions) * rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `actions` is a Tensor placeholder of shape \\[batch_size, action_dim...\\], and `rewards` is a placeholder of shape \\[batch_size\\].  \n",
    "<font color='red'> Question: why does the function gets policy as input ? I dont see it used anywhere </font>  \n",
    "The `action_dist` object is an [ActionDistribution](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#ray.rllib.models.ActionDistribution) that is parameterized by the output of the neural network policy model. Passing this loss function to `build_tf_policy` is enough to produce a very basic TF policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.tf_policy_template import build_tf_policy\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a trainer\n",
    "as an exercise (runnable file [here](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py)) we can create a [Trainer](https://ray.readthedocs.io/en/latest/rllib-concepts.html#trainers)  and try running this policy on a toy env with two parallel rollout workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "MyTrainer = build_trainer(\n",
    "    name=\"MyCustomTrainer\",\n",
    "    default_policy=MyTFPolicy)\n",
    "\n",
    "ray.init()\n",
    "tune.run(MyTrainer, config={\"env\": \"CartPole-v0\", \"num_workers\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extending with postprocessing\n",
    "if we want to compute the advantage (sum of rewards over time) we need to define a trajectory postprocessor for the policy. this can be done by defining `postprocess_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.evaluation.postprocessing import compute_advantages, \\\n",
    "    Postprocessing\n",
    "\n",
    "def postprocess_advantages(policy,\n",
    "                           sample_batch,\n",
    "                           other_agent_batches=None,\n",
    "                           episode=None):\n",
    "    return compute_advantages(\n",
    "        sample_batch, 0.0, policy.config[\"gamma\"], use_gae=False, use_critic=False)\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[SampleBatch.ACTIONS]) *\n",
    "        train_batch[Postprocessing.ADVANTAGES])\n",
    "\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss,\n",
    "    postprocess_fn=postprocess_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how RLlib makes the advantages placeholder automatically available as `train_batch[Postprocessing.ADVANTAGES]` ?\n",
    "When building your policy, RLlib will create a “dummy” trajectory batch where all observations, actions, rewards, etc. are zeros. It then calls your postprocess_fn, and generates TF placeholders based on the numpy shapes of the postprocessed batch. RLlib tracks which placeholders that `loss_fn` and `stats_fn` access, and then feeds the corresponding sample data into those placeholders during loss optimization. You can also access these placeholders via `policy.get_placeholder(<name>)` after loss initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building policies in Eager\n",
    "Policies built with `build_tf_policy` (most of the reference algorithms are) can be run in eager mode by setting the `\"eager\": True` / `\"eager_tracing\": True` config options or using `rllib train --eager [--trace]`. This will tell RLlib to execute the model forward pass, action distribution, loss, and stats functions in eager mode.\n",
    "\n",
    "Eager mode makes debugging much easier, since you can now use line-by-line debugging with breakpoints or Python `print()` to inspect intermediate tensor values. However, eager can be slower than graph mode unless tracing is enabled.\n",
    "\n",
    "You can also selectively leverage eager operations within graph mode execution with tf.py_function. Here’s [an example](https://github.com/ray-project/ray/blob/master/rllib/examples/eager_execution.py) of using eager ops embedded within a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : PPO implementation\n",
    "in this example we'll see how the above flow is used to buid the PPO trainer and how we can modify it.  \n",
    "We'll go through the [PPO trainer definition](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo.py)\n",
    "\n",
    "\n",
    "at the bottom of the file, we are using `build_trainer` to build the PPOTrainer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PPOTrainer = build_trainer(\n",
    "    name=\"PPOTrainer\",    \n",
    "    default_policy=PPOTFPolicy,\n",
    "    \n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    make_policy_optimizer=choose_policy_optimizer,\n",
    "    validate_config=validate_config,\n",
    "    after_optimizer_step=update_kl,\n",
    "    before_train_step=warn_about_obs_filter,\n",
    "    after_train_result=warn_about_bad_reward_scales)\n",
    "```\n",
    "\n",
    "Lets dive into some of the parameters used above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### choose_policy_optimizer\n",
    "this function is fed through `make_policy_optimizer` and chooses which [Policy Optimizer](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization) to use for distributed training. You can think of these policy optimizers as coordinating the distributed workflow needed to improve the policy. Depending on the trainer config, PPO can switch between a simple synchronous optimizer, or a multi-GPU optimizer that implements minibatch SGD (the default):\n",
    "\n",
    "```\n",
    "def choose_policy_optimizer(workers, config):\n",
    "    if config[\"simple_optimizer\"]:\n",
    "        return SyncSamplesOptimizer(\n",
    "            workers,\n",
    "            num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "            train_batch_size=config[\"train_batch_size\"])\n",
    "\n",
    "    return LocalMultiGPUOptimizer(\n",
    "        workers,\n",
    "        sgd_batch_size=config[\"sgd_minibatch_size\"],\n",
    "        num_sgd_iter=config[\"num_sgd_iter\"],\n",
    "        num_gpus=config[\"num_gpus\"],\n",
    "        sample_batch_size=config[\"sample_batch_size\"],\n",
    "        num_envs_per_worker=config[\"num_envs_per_worker\"],\n",
    "        train_batch_size=config[\"train_batch_size\"],\n",
    "        standardize_fields=[\"advantages\"],\n",
    "        straggler_mitigation=config[\"straggler_mitigation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to customize PPO to use an asynchronous-gradient optimization strategy similar to A3C. To do that, we could define a new function that returns `AsyncGradientsOptimizer` and override the `make_policy_optimizer` component of PPOTrainer:\n",
    "\n",
    "in the below code we'll see how to use `with_updates` to override specific fields of a predefined trainer.\n",
    "The `with_updates` method is also available for Torch and TF policies built from templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.optimizers import AsyncGradientsOptimizer\n",
    "\n",
    "def make_async_optimizer(workers, config):\n",
    "    return AsyncGradientsOptimizer(workers, grads_per_step=100)\n",
    "\n",
    "CustomTrainer = PPOTrainer.with_updates(\n",
    "    make_policy_optimizer=make_async_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update_kl\n",
    "This is used to adaptively adjust the KL penalty coefficient on the PPO loss, which bounds the policy change per training step. You’ll notice the code handles both single and multi-agent cases (where there are be multiple policies each with different KL coeffs):\n",
    "```\n",
    "def update_kl(trainer, fetches):\n",
    "    if \"kl\" in fetches:\n",
    "        # single-agent\n",
    "        trainer.workers.local_worker().for_policy(\n",
    "            lambda pi: pi.update_kl(fetches[\"kl\"]))\n",
    "    else:\n",
    "\n",
    "        def update(pi, pi_id):\n",
    "            if pi_id in fetches:\n",
    "                pi.update_kl(fetches[pi_id][\"kl\"])\n",
    "            else:\n",
    "                logger.debug(\"No data for {}, not updating kl\".format(pi_id))\n",
    "\n",
    "        # multi-agent\n",
    "        trainer.workers.local_worker().foreach_trainable_policy(update)\n",
    "```\n",
    "\n",
    "The `update_kl` method on the policy is defined in [PPOTFPolicy](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo_policy.py) via the `KLCoeffMixin`, along with several other advanced features. Let’s look at each new feature used by the policy:\n",
    "```\n",
    "PPOTFPolicy = build_tf_policy(\n",
    "    name=\"PPOTFPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.ppo.ppo.DEFAULT_CONFIG,\n",
    "    loss_fn=ppo_surrogate_loss,\n",
    "    stats_fn=kl_and_loss_stats,\n",
    "    extra_action_fetches_fn=vf_preds_and_logits_fetches,\n",
    "    postprocess_fn=postprocess_ppo_gae,\n",
    "    gradients_fn=clip_gradients,\n",
    "    before_loss_init=setup_mixins,\n",
    "    mixins=[LearningRateSchedule, KLCoeffMixin, ValueNetworkMixin])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`stats_fn`**:  \n",
    "The stats function returns a dictionary of Tensors that will be reported with the training results. This also includes the `kl` metric which is used by the trainer to adjust the KL penalty. Note that many of the values below reference `policy.loss_obj`, which is assigned by `loss_fn` (not shown here since the PPO loss is quite complex). RLlib will always call `stats_fn` after `loss_fn`, so you can rely on using values saved by `loss_fn` as part of your statistics:\n",
    "\n",
    "```\n",
    "def kl_and_loss_stats(policy, train_batch):\n",
    "    policy.explained_variance = explained_variance(\n",
    "        train_batch[Postprocessing.VALUE_TARGETS], policy.model.value_function())\n",
    "\n",
    "    stats_fetches = {\n",
    "        \"cur_kl_coeff\": policy.kl_coeff,\n",
    "        \"cur_lr\": tf.cast(policy.cur_lr, tf.float64),\n",
    "        \"total_loss\": policy.loss_obj.loss,\n",
    "        \"policy_loss\": policy.loss_obj.mean_policy_loss,\n",
    "        \"vf_loss\": policy.loss_obj.mean_vf_loss,\n",
    "        \"vf_explained_var\": policy.explained_variance,\n",
    "        \"kl\": policy.loss_obj.mean_kl,\n",
    "        \"entropy\": policy.loss_obj.mean_entropy,\n",
    "    }\n",
    "\n",
    "    return stats_fetches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`extra_action_fetches_fn`**  \n",
    "This function defines extra outputs that will be recorded when generating actions with the policy.  \n",
    "For example, this enables saving the raw policy logits in the experience batch, which e.g. means it can be referenced in the PPO loss function via `batch[BEHAVIOUR_LOGITS]`. Other values such as the current value prediction can also be emitted for debugging or optimization purposes:\n",
    "```\n",
    "def vf_preds_and_logits_fetches(policy):\n",
    "    return {\n",
    "        SampleBatch.VF_PREDS: policy.model.value_function(),\n",
    "        BEHAVIOUR_LOGITS: policy.model.last_output(),\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`gradients_fn`**  \n",
    " If defined, this function returns TF gradients for the loss function. You’d typically only want to override this to apply transformations such as gradient clipping:  \n",
    "```\n",
    "def clip_gradients(policy, optimizer, loss):\n",
    "    if policy.config[\"grad_clip\"] is not None:\n",
    "        grads = tf.gradients(loss, policy.model.trainable_variables())\n",
    "        policy.grads, _ = tf.clip_by_global_norm(grads,\n",
    "                                                 policy.config[\"grad_clip\"])\n",
    "        clipped_grads = list(zip(policy.grads, policy.model.trainable_variables()))\n",
    "        return clipped_grads\n",
    "    else:\n",
    "        return optimizer.compute_gradients(\n",
    "            loss, colocate_gradients_with_ops=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`mixings`**  \n",
    "To add arbitrary stateful components, you can add mixin classes to the policy. Methods defined by these mixins will have higher priority than the base policy class, so you can use these to override methods (as in the case of `LearningRateSchedule`), or define extra methods and attributes (e.g., `KLCoeffMixin`, `ValueNetworkMixin`). Like any other Python superclass, these should be initialized at some point, which is what the `setup_mixins` function does:  \n",
    "```\n",
    "def setup_mixins(policy, obs_space, action_space, config):\n",
    "    ValueNetworkMixin.__init__(policy, obs_space, action_space, config)\n",
    "    KLCoeffMixin.__init__(policy, config)\n",
    "    LearningRateSchedule.__init__(policy, config[\"lr\"], config[\"lr_schedule\"])\n",
    "```\n",
    "In PPO we run `setup_mixins` before the loss function is called (i.e., `before_loss_init`), but other callbacks you can use include `before_init` and `after_init`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : DQN implementation\n",
    "Let’s look at how to implement a different family of policies, by looking at the [SimpleQ policy definition](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/simple_q_policy.py):  \n",
    "(Note that this is a simplified version of [DQNTFPolicy](https://github.com/ray-project/ray/blob/master/rllib/agents/dqn/dqn_policy.py))\n",
    "\n",
    "```\n",
    "SimpleQPolicy = build_tf_policy(\n",
    "    name=\"SimpleQPolicy\",\n",
    "    get_default_config=lambda: ray.rllib.agents.dqn.dqn.DEFAULT_CONFIG,\n",
    "    make_model=build_q_models,\n",
    "    action_sampler_fn=build_action_sampler,\n",
    "    loss_fn=build_q_losses,\n",
    "    extra_action_feed_fn=exploration_setting_inputs,\n",
    "    extra_action_fetches_fn=lambda policy: {\"q_values\": policy.q_values},\n",
    "    extra_learn_fetches_fn=lambda policy: {\"td_error\": policy.td_error},\n",
    "    before_init=setup_early_mixins,\n",
    "    after_init=setup_late_mixins,\n",
    "    obs_include_prev_action_reward=False,\n",
    "    mixins=[\n",
    "        ExplorationStateMixin,\n",
    "        TargetNetworkMixin,\n",
    "    ])\n",
    "```\n",
    "\n",
    "The biggest difference from the policy gradient policies you saw previously is that SimpleQPolicy defines its own `make_model` and `action_sampler_fn`. This means that the policy builder will not internally create a model and action distribution, rather it will call `build_q_models` and `build_action_sampler` to get the output action tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`build_q_models`**  \n",
    "The model creation function actually creates two different models for DQN: the base Q network, and also a target network. It requires each model to be of type `SimpleQModel`, which implements a `get_q_values()` method. The model catalog will raise an error if you try to use a custom ModelV2 model that isn’t a subclass of `SimpleQModel`. Similarly, the full DQN policy requires models to subclass `DistributionalQModel`, which implements `get_q_value_distributions()` and `get_state_value()`:  \n",
    "```\n",
    "def build_q_models(policy, obs_space, action_space, config):\n",
    "    ...\n",
    "\n",
    "    policy.q_model = ModelCatalog.get_model_v2(\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        config[\"model\"],\n",
    "        framework=\"tf\",\n",
    "        name=Q_SCOPE,\n",
    "        model_interface=SimpleQModel,\n",
    "        q_hiddens=config[\"hiddens\"])\n",
    "\n",
    "    policy.target_q_model = ModelCatalog.get_model_v2(\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        config[\"model\"],\n",
    "        framework=\"tf\",\n",
    "        name=Q_TARGET_SCOPE,\n",
    "        model_interface=SimpleQModel,\n",
    "        q_hiddens=config[\"hiddens\"])\n",
    "\n",
    "    return policy.q_model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`action_sampler`**\n",
    "The action sampler is straightforward, it just takes the q_model, runs a forward pass, and returns the argmax over the actions:\n",
    "```\n",
    "def build_action_sampler(policy, q_model, input_dict, obs_space, action_space,\n",
    "                         config):\n",
    "    # do max over Q values...\n",
    "    ...\n",
    "    return action, action_logp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of DQN is similar to other algorithms. Target updates are handled by a `after_optimizer_step` callback that periodically copies the weights of the Q network to the target.\n",
    "\n",
    "Finally, note that you do not have to use `build_tf_policy` to define a TensorFlow policy. You can alternatively subclass `Policy`, `TFPolicy`, or `DynamicTFPolicy` as convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending existing policies\n",
    "You can use the with_updates method on Trainers and Policy objects built with `make_*` to create a copy of the object with some changes, for example:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "\n",
    "CustomPolicy = PPOTFPolicy.with_updates(\n",
    "    name=\"MyCustomPPOTFPolicy\",\n",
    "    loss_fn=some_custom_loss_fn)\n",
    "\n",
    "CustomTrainer = PPOTrainer.with_updates(\n",
    "    default_policy=CustomPolicy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "Given an environment and policy, policy evaluation produces batches of experiences. This is your classic “environment interaction loop”. Efficient policy evaluation can be burdensome to get right, especially when leveraging vectorization, RNNs, or when operating in a multi-agent environment. RLlib provides a RolloutWorker class that manages all of this, and this class is used in most RLlib algorithms.\n",
    "\n",
    "You can use rollout workers standalone to produce batches of experiences. This can be done by calling `worker.sample()` on a worker instance, or `worker.sample.remote()` in parallel on worker instances created as Ray actors (see WorkerSet).\n",
    "\n",
    "Here is an example of creating a set of rollout workers and using them gather experiences in parallel. The trajectories are concatenated, the policy learns on the trajectory batch, and then we broadcast the policy weights to the workers for the next round of rollouts:\n",
    "```\n",
    "# Setup policy and rollout workers\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "workers = WorkerSet(\n",
    "    policy=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v0\"),\n",
    "    num_workers=10)\n",
    "\n",
    "while True:\n",
    "    # Gather a batch of samples\n",
    "    T1 = SampleBatch.concat_samples(\n",
    "        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n",
    "\n",
    "    # Improve the policy using the T1 batch\n",
    "    policy.learn_on_batch(T1)\n",
    "\n",
    "    # Broadcast weights to the policy evaluation workers\n",
    "    weights = ray.put({\"default_policy\": policy.get_weights()})\n",
    "    for w in workers.remote_workers():\n",
    "        w.set_weights.remote(weights)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization  \n",
    "Similar to how a [gradient-descent optimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) can be used to improve a model, RLlib’s [policy optimizers](https://github.com/ray-project/ray/tree/master/rllib/optimizers) implement different strategies for improving a policy.\n",
    "\n",
    "For example, in A3C you’d want to compute gradients asynchronously on different workers, and apply them to a central policy replica. This strategy is implemented by the [AsyncGradientsOptimizer](https://github.com/ray-project/ray/blob/master/rllib/optimizers/async_gradients_optimizer.py). Another alternative is to gather experiences synchronously in parallel and optimize the model centrally, as in [SyncSamplesOptimizer](https://github.com/ray-project/ray/blob/master/rllib/optimizers/sync_samples_optimizer.py). Policy optimizers abstract these strategies away into reusable modules.\n",
    "\n",
    "This is how the example in the previous section looks when written using a policy optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Same setup as before\n",
    "workers = WorkerSet(\n",
    "    policy=CustomPolicy,\n",
    "    env_creator=lambda c: gym.make(\"CartPole-v0\"),\n",
    "    num_workers=10)\n",
    "\n",
    "# this optimizer implements the IMPALA architecture\n",
    "optimizer = AsyncSamplesOptimizer(workers, train_batch_size=500)\n",
    "\n",
    "while True:\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers & API\n",
    "Trainers are the boilerplate classes that put the above components together, making algorithms accessible via Python API and the command line. They manage algorithm configuration, setup of the rollout workers and optimizer, and collection of training metrics. Trainers also implement the Trainable API for easy experiment management.\n",
    "\n",
    "![rrlib_api](rllib-api.svg)\n",
    "\n",
    "Example of three equivalent ways of interacting with the PPO trainer, all of which log results in `~/ray_results`:\n",
    "\n",
    "**Method 1**\n",
    "\n",
    "```\n",
    "trainer = PPOTrainer(env=\"CartPole-v0\", config={\"train_batch_size\": 4000})\n",
    "while True:\n",
    "    print(trainer.train())\n",
    "```\n",
    "\n",
    "**Method 2**\n",
    "```\n",
    "rllib train --run=PPO --env=CartPole-v0 --config='{\"train_batch_size\": 4000}'\n",
    "```\n",
    "or \n",
    "```\n",
    "rllib train --run DQN --env CartPole-v0  # --eager [--trace] for eager execution\n",
    "```\n",
    "or if we have a tuned example we can provide the yaml file:\n",
    "```\n",
    "rllib train -f /path/to/tuned/example.yaml\n",
    "```\n",
    "running the train command is equivalent to running the `train.py` script. \n",
    "The most important options for the scripts are:  \n",
    "`--env` - for choosing the environment (any OpenAI gym environment including ones registered by the user can be used)   \n",
    "`--run` - for choosing the algorithm (available options are SAC, PPO, PG, A2C, A3C, IMPALA, ES, DDPG, DQN, MARWIL, APEX, and APEX_DDPG).\n",
    "\n",
    "\n",
    "**Method 3**  \n",
    "All RLlib trainers are compatible with the [Tune API](https://ray.readthedocs.io/en/latest/tune-usage.html). This enables them to be easily used in experiments with [Tune](https://ray.readthedocs.io/en/latest/tune.html).\n",
    "```\n",
    "from ray import tune\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\", \"train_batch_size\": 4000})\n",
    "```\n",
    "\n",
    "another example (taken from [here](https://ray.readthedocs.io/en/latest/rllib-training.html#basic-python-api)):  \n",
    "```\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "        \"eager\": False,\n",
    "    },\n",
    ")\n",
    "```\n",
    "All RLlib trainers are compatible with the Tune API. This enables them to be easily used in experiments with Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "#### Specifying parameters\n",
    "Each algorithm has specific hyperparameters that can be set with `--config`, in addition to a number of [common hyperparameters](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py). See the [algorithms documentation](https://ray.readthedocs.io/en/latest/rllib-algorithms.html) for more information.  \n",
    "\n",
    "you can also find the [common parameters in the documentation](https://ray.readthedocs.io/en/latest/rllib-training.html#common-parameters)\n",
    "\n",
    "#### Specifying Resources\n",
    "You can control the degree of parallelism used by setting the `num_workers` hyperparameter for most algorithms.  \n",
    "The number of GPUs the driver should use can be set via the `num_gpus` option.  \n",
    "Similarly, the resource allocation to workers can be controlled via `num_cpus_per_worker`, `num_gpus_per_worker`, and `custom_resources_per_worker`.  \n",
    "The number of GPUs can be a fractional quantity to allocate only a fraction of a GPU. For example, with DQN you can pack five trainers onto one GPU by setting `num_gpus: 0.2`.\n",
    "\n",
    "![rllib-config](rllib-config.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating trained policies\n",
    "In order to save checkpoints from which to evaluate policies, set `--checkpoint-freq` (number of training iterations between checkpoints) when running `rllib train`.\n",
    "\n",
    "An example of evaluating a previously trained DQN policy is as follows:\n",
    "```\n",
    "rllib rollout \\\n",
    "    ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "```\n",
    "\n",
    "For more advanced evaluation functionality, refer to [Customized Evaluation During Training](https://ray.readthedocs.io/en/latest/rllib-training.html#customized-evaluation-during-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Python API\n",
    "The Python API provides the needed flexibility for applying RLlib to new problems. You will need to use this API if you wish to use custom environments, preprocessors, or models with RLlib.\n",
    "\n",
    "### Training \n",
    "Here is an example of the basic usage (for a more complete example, see custom_env.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init()\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"eager\"] = False\n",
    "trainer = ppo.PPOTrainer(config=config, env=\"CartPole-v0\")\n",
    "\n",
    "# Can optionally call trainer.restore(path) to load a checkpoint.\n",
    "\n",
    "for i in range(1000):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = trainer.train()\n",
    "   print(pretty_print(result))\n",
    "\n",
    "   if i % 100 == 0:\n",
    "       checkpoint = trainer.save()\n",
    "       print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "It’s recommended that you run RLlib trainers with [Tune](https://ray.readthedocs.io/en/latest/tune.html), for easy experiment management and visualization of results. Just set `\"run\": ALG_NAME, \"env\": ENV_NAME` in the experiment config: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "        \"eager\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Actions\n",
    "The simplest way to programmatically compute actions from a trained agent is to use `trainer.compute_action()`. This method preprocesses and filters the observation before passing it to the agent policy. For more advanced usage, you can access the workers and policies held by the trainer directly as `compute_action()` does:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Trainable):\n",
    "\n",
    "    @PublicAPI\n",
    "    def compute_action(self,\n",
    "                     observation,\n",
    "                     state=None,\n",
    "                     prev_action=None,\n",
    "                     prev_reward=None,\n",
    "                     info=None,\n",
    "                     policy_id=DEFAULT_POLICY_ID,\n",
    "                     full_fetch=False):\n",
    "        \"\"\"Computes an action for the specified policy.\n",
    "\n",
    "      Note that you can also access the policy object through\n",
    "      self.get_policy(policy_id) and call compute_actions() on it directly.\n",
    "\n",
    "      Arguments:\n",
    "          observation (obj): observation from the environment.\n",
    "          state (list): RNN hidden state, if any. If state is not None,\n",
    "                        then all of compute_single_action(...) is returned\n",
    "                        (computed action, rnn state, logits dictionary).\n",
    "                        Otherwise compute_single_action(...)[0] is\n",
    "                        returned (computed action).\n",
    "          prev_action (obj): previous action value, if any\n",
    "          prev_reward (int): previous reward, if any\n",
    "          info (dict): info object, if any\n",
    "          policy_id (str): policy to query (only applies to multi-agent).\n",
    "          full_fetch (bool): whether to return extra action fetch results.\n",
    "              This is always set to true if RNN state is specified.\n",
    "\n",
    "      Returns:\n",
    "          Just the computed action if full_fetch=False, or the full output\n",
    "          of policy.compute_actions() otherwise.\n",
    "      \"\"\"\n",
    "\n",
    "        if state is None:\n",
    "            state = []\n",
    "        preprocessed = self.workers.local_worker().preprocessors[policy_id].transform(observation)\n",
    "        filtered_obs = self.workers.local_worker().filters[policy_id](preprocessed, update=False)\n",
    "        if state:\n",
    "            return self.get_policy(policy_id).compute_single_action(\n",
    "              filtered_obs,\n",
    "              state,\n",
    "              prev_action,\n",
    "              prev_reward,\n",
    "              info,\n",
    "              clip_actions=self.config[\"clip_actions\"])\n",
    "        res = self.get_policy(policy_id).compute_single_action(\n",
    "          filtered_obs,\n",
    "          state,\n",
    "          prev_action,\n",
    "          prev_reward,\n",
    "          info,\n",
    "          clip_actions=self.config[\"clip_actions\"])\n",
    "        if full_fetch:\n",
    "            return res\n",
    "        else:\n",
    "            return res[0]  # backwards compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing policy states\n",
    "It is common to need to access a trainer’s internal state, e.g., to set or get internal weights. In RLlib trainer state is replicated across multiple rollout workers (Ray actors) in the cluster. However, you can easily get and update this state between calls to `train()` via `trainer.workers.foreach_worker()` or `trainer.workers.foreach_worker_with_index()`. These functions take a lambda function that is applied with the worker as an arg. You can also return values from these functions and those will be returned as a list.\n",
    "\n",
    "You can also access just the “master” copy of the trainer state through `trainer.get_policy()` or `trainer.workers.local_worker()`, but note that updates here may not be immediately reflected in remote replicas if you have configured `num_workers > 0`. For example, to access the weights of a local TF policy, you can run `trainer.get_policy().get_weights()`. This is also equivalent to `trainer.workers.local_worker().policy_map[\"default_policy\"].get_weights()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights of the default local policy\n",
    "trainer.get_policy().get_weights()\n",
    "\n",
    "# Same as above\n",
    "trainer.workers.local_worker().policy_map[\"default_policy\"].get_weights()\n",
    "\n",
    "# Get list of weights of each worker, including remote replicas\n",
    "trainer.workers.foreach_worker(lambda ev: ev.get_policy().get_weights())\n",
    "\n",
    "# Same as above\n",
    "trainer.workers.foreach_worker_with_index(lambda ev, i: ev.get_policy().get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Model State\n",
    "Similar to accessing policy state, you may want to get a reference to the underlying neural network model being trained. For example, you may want to pre-train it separately, or otherwise update its weights outside of RLlib. This can be done by accessing the `model` of the policy:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Preprocessing observations for feeding into a model\n",
    "```\n",
    ">>> import gym\n",
    ">>> env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# RLlib uses preprocessors to implement transforms such as one-hot encoding\n",
    "# and flattening of tuple and dict observations.\n",
    ">>> from ray.rllib.models.preprocessors import get_preprocessor\n",
    ">>> prep = get_preprocessor(env.observation_space)(env.observation_space)\n",
    "<ray.rllib.models.preprocessors.GenericPixelPreprocessor object at 0x7fc4d049de80>\n",
    "\n",
    "# Observations should be preprocessed prior to feeding into a model\n",
    ">>> env.reset().shape\n",
    "(210, 160, 3)\n",
    ">>> prep.transform(env.reset()).shape\n",
    "(84, 84, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Querying a policy’s action distribution\n",
    "```\n",
    "# Get a reference to the policy\n",
    ">>> from ray.rllib.agents.ppo import PPOTrainer\n",
    ">>> trainer = PPOTrainer(env=\"CartPole-v0\", config={\"eager\": True, \"num_workers\": 0})\n",
    ">>> policy = trainer.get_policy()\n",
    "<ray.rllib.policy.eager_tf_policy.PPOTFPolicy_eager object at 0x7fd020165470>\n",
    "\n",
    "# Run a forward pass to get model output logits. Note that complex observations\n",
    "# must be preprocessed as in the above code block.\n",
    ">>> logits, _ = policy.model.from_batch({\"obs\": np.array([[0.1, 0.2, 0.3, 0.4]])})\n",
    "(<tf.Tensor: id=1274, shape=(1, 2), dtype=float32, numpy=...>, [])\n",
    "\n",
    "# Compute action distribution given logits\n",
    ">>> policy.dist_class\n",
    "<class_object 'ray.rllib.models.tf.tf_action_dist.Categorical'>\n",
    ">>> dist = policy.dist_class(logits, policy.model)\n",
    "<ray.rllib.models.tf.tf_action_dist.Categorical object at 0x7fd02301d710>\n",
    "\n",
    "# Query the distribution for samples, sample logps\n",
    ">>> dist.sample()\n",
    "<tf.Tensor: id=661, shape=(1,), dtype=int64, numpy=..>\n",
    ">>> dist.logp([1])\n",
    "<tf.Tensor: id=1298, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    "# Get the estimated values for the most recent forward pass\n",
    ">>> policy.model.value_function()\n",
    "<tf.Tensor: id=670, shape=(1,), dtype=float32, numpy=...>\n",
    "\n",
    ">>> policy.model.base_model.summary()\n",
    "Model: \"model\"\n",
    "_____________________________________________________________________\n",
    "Layer (type)               Output Shape  Param #  Connected to\n",
    "=====================================================================\n",
    "observations (InputLayer)  [(None, 4)]   0\n",
    "_____________________________________________________________________\n",
    "fc_1 (Dense)               (None, 256)   1280     observations[0][0]\n",
    "_____________________________________________________________________\n",
    "fc_value_1 (Dense)         (None, 256)   1280     observations[0][0]\n",
    "_____________________________________________________________________\n",
    "fc_2 (Dense)               (None, 256)   65792    fc_1[0][0]\n",
    "_____________________________________________________________________\n",
    "fc_value_2 (Dense)         (None, 256)   65792    fc_value_1[0][0]\n",
    "_____________________________________________________________________\n",
    "fc_out (Dense)             (None, 2)     514      fc_2[0][0]\n",
    "_____________________________________________________________________\n",
    "value_out (Dense)          (None, 1)     257      fc_value_2[0][0]\n",
    "=====================================================================\n",
    "Total params: 134,915\n",
    "Trainable params: 134,915\n",
    "Non-trainable params: 0\n",
    "_____________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Getting Q values from a DQN model**\n",
    "```\n",
    "# Get a reference to the model through the policy\n",
    ">>> from ray.rllib.agents.dqn import DQNTrainer\n",
    ">>> trainer = DQNTrainer(env=\"CartPole-v0\", config={\"eager\": True})\n",
    ">>> model = trainer.get_policy().model\n",
    "<ray.rllib.models.catalog.FullyConnectedNetwork_as_DistributionalQModel ...>\n",
    "\n",
    "# List of all model variables\n",
    ">>> model.variables()\n",
    "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 256) dtype=float32>, ...]\n",
    "\n",
    "# Run a forward pass to get base model output. Note that complex observations\n",
    "# must be preprocessed. An example of preprocessing is examples/saving_experiences.py\n",
    ">>> model_out = model.from_batch({\"obs\": np.array([[0.1, 0.2, 0.3, 0.4]])})\n",
    "(<tf.Tensor: id=832, shape=(1, 256), dtype=float32, numpy=...)\n",
    "\n",
    "# Access the base Keras models (all default models have a base)\n",
    ">>> model.base_model.summary()\n",
    "Model: \"model\"\n",
    "_______________________________________________________________________\n",
    "Layer (type)                Output Shape    Param #  Connected to\n",
    "=======================================================================\n",
    "observations (InputLayer)   [(None, 4)]     0\n",
    "_______________________________________________________________________\n",
    "fc_1 (Dense)                (None, 256)     1280     observations[0][0]\n",
    "_______________________________________________________________________\n",
    "fc_out (Dense)              (None, 256)     65792    fc_1[0][0]\n",
    "_______________________________________________________________________\n",
    "value_out (Dense)           (None, 1)       257      fc_1[0][0]\n",
    "=======================================================================\n",
    "Total params: 67,329\n",
    "Trainable params: 67,329\n",
    "Non-trainable params: 0\n",
    "______________________________________________________________________________\n",
    "\n",
    "# Access the Q value model (specific to DQN)\n",
    ">>> model.get_q_value_distributions(model_out)\n",
    "[<tf.Tensor: id=891, shape=(1, 2)>, <tf.Tensor: id=896, shape=(1, 2, 1)>]\n",
    "\n",
    ">>> model.q_value_head.summary()\n",
    "Model: \"model_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "model_out (InputLayer)       [(None, 256)]             0\n",
    "_________________________________________________________________\n",
    "lambda (Lambda)              [(None, 2), (None, 2, 1), 66306\n",
    "=================================================================\n",
    "Total params: 66,306\n",
    "Trainable params: 66,306\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "# Access the state value model (specific to DQN)\n",
    ">>> model.get_state_value(model_out)\n",
    "<tf.Tensor: id=913, shape=(1, 1), dtype=float32>\n",
    "\n",
    ">>> model.state_value_head.summary()\n",
    "Model: \"model_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "model_out (InputLayer)       [(None, 256)]             0\n",
    "_________________________________________________________________\n",
    "lambda_1 (Lambda)            (None, 1)                 66049\n",
    "=================================================================\n",
    "Total params: 66,049\n",
    "Trainable params: 66,049\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "This is especially useful when used with [custom model classes](https://ray.readthedocs.io/en/latest/rllib-models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced Python API](https://ray.readthedocs.io/en/latest/rllib-training.html#advanced-python-apis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training Workflows\n",
    "In the [basic training example](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py), Tune will call `train()` on your trainer once per training iteration and report the new training results. Sometimes, it is desirable to have full control over training, but still run inside Tune. Tune supports [custom trainable functions](https://ray.readthedocs.io/en/latest/tune-usage.html#trainable-api) that can be used to implement [custom training workflows (example)](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_train_fn.py).\n",
    "\n",
    "For even finer-grained control over training, you can use RLlib’s lower-level [building blocks](https://ray.readthedocs.io/en/latest/rllib-concepts.html) directly to implement [fully customized training workflows](https://github.com/ray-project/ray/blob/master/rllib/examples/rollout_worker_custom_workflow.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Coordination\n",
    "read more details [here](https://ray.readthedocs.io/en/latest/rllib-training.html#global-coordination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks and Custom Metrics\n",
    "You can provide callback functions to be called at points during policy evaluation. These functions have access to an info dict containing state for the current [episode](https://github.com/ray-project/ray/blob/master/rllib/evaluation/episode.py). Custom state can be stored for the episode in the `info[\"episode\"].user_data` dict, and custom scalar metrics reported by saving values to the `info[\"episode\"].custom_metrics` dict. These custom metrics will be aggregated and reported as part of training results. The following example (full code [here](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_metrics_and_callbacks.py)) logs a custom metric from the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_episode_start(info):\n",
    "    print(info.keys())  # -> \"env\", 'episode\"\n",
    "    episode = info[\"episode\"]\n",
    "    print(\"episode {} started\".format(episode.episode_id))\n",
    "    episode.user_data[\"pole_angles\"] = []\n",
    "\n",
    "def on_episode_step(info):\n",
    "    episode = info[\"episode\"]\n",
    "    pole_angle = abs(episode.last_observation_for()[2])\n",
    "    episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "\n",
    "def on_episode_end(info):\n",
    "    episode = info[\"episode\"]\n",
    "    pole_angle = np.mean(episode.user_data[\"pole_angles\"])\n",
    "    print(\"episode {} ended with length {} and pole angles {}\".format(\n",
    "        episode.episode_id, episode.length, pole_angle))\n",
    "    episode.custom_metrics[\"pole_angle\"] = pole_angle\n",
    "\n",
    "def on_train_result(info):\n",
    "    print(\"trainer.train() result: {} -> {} episodes\".format(\n",
    "        info[\"trainer\"].__name__, info[\"result\"][\"episodes_this_iter\"]))\n",
    "\n",
    "def on_postprocess_traj(info):\n",
    "    episode = info[\"episode\"]\n",
    "    batch = info[\"post_batch\"]  # note: you can mutate this\n",
    "    print(\"postprocessed {} steps\".format(batch.count))\n",
    "\n",
    "ray.init()\n",
    "analysis = tune.run(\n",
    "    \"PG\",\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"callbacks\": {\n",
    "            \"on_episode_start\": on_episode_start,\n",
    "            \"on_episode_step\": on_episode_step,\n",
    "            \"on_episode_end\": on_episode_end,\n",
    "            \"on_train_result\": on_train_result,\n",
    "            \"on_postprocess_traj\": on_postprocess_traj,\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Exploration Behavior (Training and Evaluation)\n",
    "RLlib offers a unified top-level API to configure and customize an agent’s exploration behavior, including the decisions (how and whether) to sample actions from distributions (stochastically or deterministically). The setup can be done via using built-in Exploration classes (see this [package](https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/)), which are specified (and further configured) inside `Trainer.config[\"exploration_config\"]`. Besides using built-in classes, one can sub-class any of these built-ins, add custom behavior to it, and use that new class in the config instead.\n",
    "\n",
    "Every policy has-an instantiation of one of the Exploration (sub-)classes. This Exploration object is created from the Trainer’s `config[“exploration_config”]` dict, which specifies the class to use via the special “type” key, as well as constructor arguments via all other keys, e.g.:\n",
    "```\n",
    "# in Trainer.config:\n",
    "\"exploration_config\": {\n",
    "    \"type\": \"StochasticSampling\",  # <- Special `type` key provides class information\n",
    "    \"[c'tor arg]\" : \"[value]\",  # <- Add any needed constructor args here.\n",
    "    # etc\n",
    "}\n",
    "# ...\n",
    "```\n",
    "The following table lists all built-in Exploration sub-classes and the agents that currently used these by default:\n",
    "![exploration_api](rllib-exploration-api-table.svg)\n",
    "\n",
    "An Exploration class implements the `get_exploration_action` method, in which the exact exploratory behavior is defined. It takes the model’s output, the action distribution class, the model itself, a timestep (the global env-sampling steps already taken), and an `explore` switch and outputs a tuple of 1) action and 2) log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exploration_action(self,\n",
    "                           distribution_inputs,\n",
    "                           action_dist_class,\n",
    "                           model=None,\n",
    "                           explore=True,\n",
    "                           timestep=None):\n",
    "    \"\"\"Returns a (possibly) exploratory action and its log-likelihood.\n",
    "\n",
    "    Given the Model's logits outputs and action distribution, returns an\n",
    "    exploratory action.\n",
    "\n",
    "    Args:\n",
    "        distribution_inputs (any): The output coming from the model,\n",
    "            ready for parameterizing a distribution\n",
    "            (e.g. q-values or PG-logits).\n",
    "        action_dist_class (class): The action distribution class\n",
    "            to use.\n",
    "        model (ModelV2): The Model object.\n",
    "        explore (bool): True: \"Normal\" exploration behavior.\n",
    "            False: Suppress all exploratory behavior and return\n",
    "                a deterministic action.\n",
    "        timestep (int): The current sampling time step. If None, the\n",
    "            component should try to use an internal counter, which it\n",
    "            then increments by 1. If provided, will set the internal\n",
    "            counter to the given value.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "        - The chosen exploration action or a tf-op to fetch the exploration\n",
    "          action from the graph.\n",
    "        - The log-likelihood of the exploration action.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the highest level, the `Trainer.compute_action` and `Policy.compute_action(s)` methods have a boolean explore switch, which is passed into `Exploration.get_exploration_action`. If None, the value of `Trainer.config[“explore”]` is used. Hence `config[“explore”]` describes the default behavior of the policy and e.g. allows switching off any exploration easily for evaluation purposes (see [Customized Evaluation During Training](https://ray.readthedocs.io/en/latest/rllib-training.html#customevaluation)).\n",
    "\n",
    "The following are example excerpts from different Trainers’ configs (see rllib/agents/trainer.py) to setup different exploration behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the following configs go into Trainer.config.\n",
    "\n",
    "# 1) Switching *off* exploration by default.\n",
    "# Behavior: Calling `compute_action(s)` without explicitly setting its `explore`\n",
    "# param will result in no exploration.\n",
    "# However, explicitly calling `compute_action(s)` with `explore=True` will\n",
    "# still(!) result in exploration (per-call overrides default).\n",
    "\"explore\": False,\n",
    "\n",
    "# 2) Switching *on* exploration by default.\n",
    "# Behavior: Calling `compute_action(s)` without explicitly setting its\n",
    "# explore param will result in exploration.\n",
    "# However, explicitly calling `compute_action(s)` with `explore=False`\n",
    "# will result in no(!) exploration (per-call overrides default).\n",
    "\"explore\": True,\n",
    "\n",
    "# 3) Example exploration_config usages:\n",
    "# a) DQN: see rllib/agents/dqn/dqn.py\n",
    "\"explore\": True,\n",
    "\"exploration_config\": {\n",
    "   \"type\": \"EpsilonGreedy\",  # <- Exploration sub-class by name or full path to module+class\n",
    "                             # (e.g. “ray.rllib.utils.exploration.epsilon_greedy.EpsilonGreedy”)\n",
    "   # Parameters for the Exploration class' constructor:\n",
    "   \"initial_epsilon\": 1.0,\n",
    "   \"final_epsilon\": 0.02,\n",
    "   \"epsilon_timesteps\": 10000,  # Timesteps over which to anneal epsilon.\n",
    "},\n",
    "\n",
    "# b) DQN Soft-Q: In order to switch to Soft-Q exploration, do instead:\n",
    "\"explore\": True,\n",
    "\"exploration_config\": {\n",
    "   \"type\": \"SoftQ\",\n",
    "   # Parameters for the Exploration class' constructor:\n",
    "   \"temperature\": 1.0,\n",
    "},\n",
    "\n",
    "# c) PPO: see rllib/agents/ppo/ppo.py\n",
    "# Behavior: The algo samples stochastically by default from the\n",
    "# model-parameterized distribution. This is the global Trainer default\n",
    "# setting defined in trainer.py and used by all PG-type algos.\n",
    "\"explore\": True,\n",
    "\"exploration_config\": {\n",
    "   \"type\": \"StochasticSampling\",\n",
    "},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Evaluation During Training\n",
    "RLlib will report online training rewards, however in some cases you may want to compute rewards with different settings (e.g., with exploration turned off, or on a specific set of environment configurations). You can evaluate policies during training by setting one or more of the `evaluation_interval`, `evaluation_num_episodes`, `evaluation_config`, `evaluation_num_workers`, and `custom_eval_function` configs (see trainer.py for further documentation).\n",
    "\n",
    "By default, exploration is left as-is within `evaluation_config`. However, you can switch off any exploration behavior for the evaluation workers via:\n",
    "```\n",
    "# Switching off exploration behavior for evaluation workers\n",
    "# (see rllib/agents/trainer.py)\n",
    "\"evaluation_config\": {\n",
    "   \"explore\": False\n",
    "}\n",
    "```\n",
    "**IMPORTANT NOTE**: Policy gradient algorithms are able to find the optimal policy, even if this is a stochastic one. Setting `“explore=False”` above will result in the evaluation workers not using this optimal policy.\n",
    "\n",
    "There is an end to end example of how to set up custom online evaluation in [custom_eval.py](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_eval.py). Note that if you only want to eval your policy at the end of training, you can set evaluation_interval: N, where N is the number of training iterations before stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting Trajectories\n",
    "Note that in the `on_postprocess_traj` callback you have full access to the trajectory batch (`post_batch`) and other training state. This can be used to rewrite the trajectory, which has a number of uses including:\n",
    "\n",
    "- Backdating rewards to previous time steps (e.g., based on values in `info`).\n",
    "- Adding model-based curiosity bonuses to rewards (you can train the model with a custom model supervised loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning\n",
    "Let’s look at two ways to use the above APIs to implement [curriculum learning](https://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/). In curriculum learning, the agent task is adjusted over time to improve the learning process. Suppose that we have an environment class with a `set_phase()` method that we can call to adjust the task difficulty over time:\n",
    "\n",
    "**Approach 1**: Use the Trainer API and update the environment between calls to `train()`. This example shows the trainer being run inside a Tune function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = PPOTrainer(config=config, env=YourEnv)\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "        if result[\"episode_reward_mean\"] > 200:\n",
    "            phase = 2\n",
    "        elif result[\"episode_reward_mean\"] > 100:\n",
    "            phase = 1\n",
    "        else:\n",
    "            phase = 0\n",
    "        trainer.workers.foreach_worker(\n",
    "            lambda ev: ev.foreach_env(\n",
    "                lambda env: env.set_phase(phase)))\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    train,\n",
    "    config={\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": lambda spec: spec.config.num_gpus,\n",
    "        \"extra_cpu\": lambda spec: spec.config.num_workers,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2**: Use the callbacks API to update the environment on new training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    if result[\"episode_reward_mean\"] > 200:\n",
    "        phase = 2\n",
    "    elif result[\"episode_reward_mean\"] > 100:\n",
    "        phase = 1\n",
    "    else:\n",
    "        phase = 0\n",
    "    trainer = info[\"trainer\"]\n",
    "    trainer.workers.foreach_worker(\n",
    "        lambda ev: ev.foreach_env(\n",
    "            lambda env: env.set_phase(phase)))\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config={\n",
    "        \"env\": YourEnv,\n",
    "        \"callbacks\": {\n",
    "            \"on_train_result\": on_train_result,\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym Monitor\n",
    "The `\"monitor\": true` config can be used to save Gym episode videos to the result dir. For example:\n",
    "\n",
    "```\n",
    "rllib train --env=PongDeterministic-v4 \\\n",
    "    --run=A2C --config '{\"num_workers\": 2, \"monitor\": true}'\n",
    "\n",
    "# videos will be saved in the ~/ray_results/<experiment> dir, for example\n",
    "openaigym.video.0.31401.video000000.meta.json\n",
    "openaigym.video.0.31401.video000000.mp4\n",
    "openaigym.video.0.31403.video000000.meta.json\n",
    "openaigym.video.0.31403.video000000.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Mode\n",
    "Policies built with `build_tf_policy` (most of the reference algorithms are) can be run in eager mode by setting the `\"eager\": True` / `\"eager_tracing\": True` config options or using `rllib train --eager [--trace]`. This will tell RLlib to execute the model forward pass, action distribution, loss, and stats functions in eager mode.\n",
    "\n",
    "Eager mode makes debugging much easier, since you can now use line-by-line debugging with breakpoints or Python `print()` to inspect intermediate tensor values. However, eager can be slower than graph mode unless tracing is enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Traces\n",
    "You can use the data output API to save episode traces for debugging. For example, the following command will run PPO while saving episode traces to `/tmp/debug`:\n",
    "```\n",
    "rllib train --run=PPO --env=CartPole-v0 \\\n",
    "    --config='{\"output\": \"/tmp/debug\", \"output_compress_columns\": []}'\n",
    "\n",
    "# episode traces will be saved in /tmp/debug, for example\n",
    "output-2019-02-23_12-02-03_worker-2_0.json\n",
    "output-2019-02-23_12-02-04_worker-1_0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Verbosity\n",
    "You can control the trainer log level via the `\"log_level\"` flag. Valid values are `“DEBUG”`, `“INFO”`, `“WARN”` (default), and `“ERROR”`. This can be used to increase or decrease the verbosity of internal logging. You can also use the `-v` and `-vv` flags.  \n",
    "For example, the following two commands are about equivalent:\n",
    "\n",
    "```\n",
    "rllib train --env=PongDeterministic-v4 --run=A2C --config '{\"num_workers\": 2, \"log_level\": \"DEBUG\"}'\n",
    "\n",
    "rllib train --env=PongDeterministic-v4 --run=A2C --config '{\"num_workers\": 2}' -vv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Traces\n",
    "You can use the `ray stack` command to dump the stack traces of all the Python workers on a single node. This can be useful for debugging unexpected hangs or performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API\n",
    "In some cases (i.e., when interacting with an externally hosted simulator or production environment) it makes more sense to interact with RLlib as if were an independently running service, rather than RLlib hosting the simulations itself. This is possible via RLlib’s external agents [interface](https://ray.readthedocs.io/en/latest/rllib-env.html#interfacing-with-external-agents)\n",
    "\n",
    "For a full client / server example that you can run, see the example [client script](https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_client.py) and also the corresponding [server script](https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_server.py), here configured to serve a policy for the toy CartPole-v0 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch RL ([Offline Datasets](https://ray.readthedocs.io/en/latest/rllib-offline.html#rllib-offline-datasets))\n",
    "\n",
    "RLlib’s offline dataset APIs enable working with experiences read from offline storage (e.g., disk, cloud storage, streaming systems, HDFS). For example, you might want to read experiences saved from previous training runs, or gathered from policies deployed in [web applications](https://arxiv.org/abs/1811.00260). You can also log new agent experiences produced during online training for future use.\n",
    "\n",
    "RLlib represents trajectory sequences (i.e., (s, a, r, s', ...) tuples) with [SampleBatch](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py) objects. Using a batch format enables efficient encoding and compression of experiences. During online training, RLlib uses [policy evaluation](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation) actors to generate batches of experiences in parallel using the current policy. RLlib also uses this same batch format for reading and writing experiences to offline storage.\n",
    "\n",
    "\n",
    "**Example: Training on previously saved experiences**\n",
    "For custom models and enviroments, you’ll need to use the [Python API](https://ray.readthedocs.io/en/latest/rllib-training.html#basic-python-api).\n",
    "\n",
    "In this example, we will save batches of experiences generated during online training to disk, and then leverage this saved data to train a policy offline using DQN. First, we run a simple policy gradient algorithm for 100k steps with `\"output\": \"/tmp/cartpole-out\"` to tell RLlib to write simulation outputs to the `/tmp/cartpole-out` directory.\n",
    "\n",
    "```\n",
    "$ rllib train\n",
    "    --run=PG \\\n",
    "    --env=CartPole-v0 \\\n",
    "    --config='{\"output\": \"/tmp/cartpole-out\", \"output_max_file_size\": 5000000}' \\\n",
    "    --stop='{\"timesteps_total\": 100000}'\n",
    "```\n",
    "\n",
    "The experiences will be saved in compressed JSON batch format:\n",
    "\n",
    "```\n",
    "$ ls -l /tmp/cartpole-out\n",
    "total 11636\n",
    "-rw-rw-r-- 1 eric eric 5022257 output-2019-01-01_15-58-57_worker-0_0.json\n",
    "-rw-rw-r-- 1 eric eric 5002416 output-2019-01-01_15-59-22_worker-0_1.json\n",
    "-rw-rw-r-- 1 eric eric 1881666 output-2019-01-01_15-59-47_worker-0_2.json\n",
    "```\n",
    "\n",
    "Then, we can tell DQN to train using these previously generated experiences with `\"input\": \"/tmp/cartpole-out\"`. We disable exploration since it has no effect on the input:\n",
    "\n",
    "```\n",
    "$ rllib train \\\n",
    "    --run=DQN \\\n",
    "    --env=CartPole-v0 \\\n",
    "    --config='{\n",
    "        \"input\": \"/tmp/cartpole-out\",\n",
    "        \"input_evaluation\": [],\n",
    "        \"explore\": false}'\n",
    "```\n",
    "**Off-Policy estimation**\n",
    "Since the input experiences are not from running simulations, RLlib cannot report the true policy performance during training. However, you can use tensorboard `--logdir=~/ray_results` to monitor training progress via other metrics such as estimated Q-value. Alternatively, off-policy estimation can be used, which requires both the source and target action probabilities to be available (i.e., the `action_prob` batch key). For DQN, this means enabling soft Q learning so that actions are sampled from a probability distribution:\n",
    "\n",
    "```\n",
    "$ rllib train \\\n",
    "    --run=DQN \\\n",
    "    --env=CartPole-v0 \\\n",
    "    --config='{\n",
    "        \"input\": \"/tmp/cartpole-out\",\n",
    "        \"input_evaluation\": [\"is\", \"wis\"],\n",
    "        \"exploration_config\": {\n",
    "            \"type\": \"SoftQ\",\n",
    "            \"temperature\": 1.0,\n",
    "        }'\n",
    "```\n",
    "This example plot shows the Q-value metric in addition to importance sampling (IS) and weighted importance sampling (WIS) gain estimates (>1.0 means there is an estimated improvement over the original policy):\n",
    "![offline_q](offline-q.png)\n",
    "\n",
    "**Estimator Python API**: For greater control over the evaluation process, you can create off-policy estimators in your Python code and call `estimator.estimate(episode_batch)` to perform counterfactual estimation as needed. The estimators take in a policy object and gamma value for the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(...)\n",
    "...  # train policy offline\n",
    "\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.rllib.offline.wis_estimator import WeightedImportanceSamplingEstimator\n",
    "\n",
    "estimator = WeightedImportanceSamplingEstimator(trainer.get_policy(), gamma=0.99)\n",
    "reader = JsonReader(\"/path/to/data\")\n",
    "for _ in range(1000):\n",
    "    batch = reader.next()\n",
    "    for episode in batch.split_by_episode():\n",
    "        print(estimator.estimate(episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation-based estimation**: If true simulation is also possible (i.e., your env supports `step()`), you can also set `\"input_evaluation\": [\"simulation\"]` to tell RLlib to run background simulations to estimate current policy performance. The output of these simulations will not be used for learning. Note that in all cases you still need to specify an environment object to define the action and observation spaces. However, you don’t need to implement functions like reset() and step()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experience files\n",
    "as written above, the experience buffers of each worker are saved in a json file using the [JsonWriter](https://github.com/ray-project/ray/blob/master/rllib/offline/json_writer.py) class.\n",
    "\n",
    "If we want to have a look at the content of this json file we can use the [JsonReader](https://github.com/ray-project/ray/blob/master/rllib/offline/json_reader.py) class as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t', 'eps_id', 'agent_index', 'obs', 'actions', 'rewards', 'prev_actions', 'prev_rewards', 'dones', 'infos', 'new_obs', 'action_prob', 'action_logp', 'unroll_id', 'advantages', 'value_targets'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.offline import JsonReader\n",
    "jsons_path = '/home/guy/share/Data/MLA/ray/cartpole-out/*.json'\n",
    "reader=JsonReader(jsons_path)\n",
    "batch=reader.next()\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': (numpy.ndarray, (200,)),\n",
       " 'eps_id': (numpy.ndarray, (200,)),\n",
       " 'agent_index': (numpy.ndarray, (200,)),\n",
       " 'obs': (numpy.ndarray, (200, 4)),\n",
       " 'actions': (numpy.ndarray, (200,)),\n",
       " 'rewards': (numpy.ndarray, (200,)),\n",
       " 'prev_actions': (numpy.ndarray, (200,)),\n",
       " 'prev_rewards': (numpy.ndarray, (200,)),\n",
       " 'dones': (numpy.ndarray, (200,)),\n",
       " 'infos': (numpy.ndarray, (200,)),\n",
       " 'new_obs': (numpy.ndarray, (200, 4)),\n",
       " 'action_prob': (numpy.ndarray, (200,)),\n",
       " 'action_logp': (numpy.ndarray, (200,)),\n",
       " 'unroll_id': (numpy.ndarray, (200,)),\n",
       " 'advantages': (numpy.ndarray, (200,)),\n",
       " 'value_targets': (numpy.ndarray, (200,))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:(type(v),v.shape) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Converting external experiences to batch format**\n",
    "When the env does not support simulation (e.g., it is a web application), it is necessary to generate the `*.json` experience batch files outside of RLlib. This can be done by using the [JsonWriter](https://github.com/ray-project/ray/blob/master/rllib/offline/json_writer.py) class to write out batches. This [runnable example](https://github.com/ray-project/ray/blob/master/rllib/examples/saving_experiences.py) shows how to generate and save experience batches for CartPole-v0 to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_builder = SampleBatchBuilder()  # or MultiAgentSampleBatchBuilder\n",
    "    writer = JsonWriter(\"/tmp/demo-out\")\n",
    "\n",
    "    # You normally wouldn't want to manually create sample batches if a\n",
    "    # simulator is available, but let's do it anyways for example purposes:\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    # RLlib uses preprocessors to implement transforms such as one-hot encoding\n",
    "    # and flattening of tuple and dict observations. For CartPole a no-op\n",
    "    # preprocessor is used, but this may be relevant for more complex envs.\n",
    "    prep = get_preprocessor(env.observation_space)(env.observation_space)\n",
    "    print(\"The preprocessor is\", prep)\n",
    "\n",
    "    for eps_id in range(100):\n",
    "        obs = env.reset()\n",
    "        prev_action = np.zeros_like(env.action_space.sample())\n",
    "        prev_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, rew, done, info = env.step(action)\n",
    "            batch_builder.add_values(\n",
    "                t=t,\n",
    "                eps_id=eps_id,\n",
    "                agent_index=0,\n",
    "                obs=prep.transform(obs),\n",
    "                actions=action,\n",
    "                action_prob=1.0,  # put the true action probability here\n",
    "                rewards=rew,\n",
    "                prev_actions=prev_action,\n",
    "                prev_rewards=prev_reward,\n",
    "                dones=done,\n",
    "                infos=info,\n",
    "                new_obs=prep.transform(new_obs))\n",
    "            obs = new_obs\n",
    "            prev_action = action\n",
    "            prev_reward = rew\n",
    "            t += 1\n",
    "        writer.write(batch_builder.build_and_reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On-policy algorithms and experience postprocessing**  \n",
    "RLlib assumes that input batches are of [postprocessed experiences](https://github.com/ray-project/ray/blob/b8a9e3f1064c6f8d754884fd9c75e0b2f88df4d6/rllib/policy/policy.py#L103). This isn’t typically critical for off-policy algorithms (e.g., DQN’s [post-processing](https://github.com/ray-project/ray/blob/b8a9e3f1064c6f8d754884fd9c75e0b2f88df4d6/rllib/agents/dqn/dqn_policy.py#L514) is only needed if `n_step > 1` or `worker_side_prioritization: True`). For off-policy algorithms, you can also safely set the `postprocess_inputs: True` config to auto-postprocess data.\n",
    "\n",
    "However, for on-policy algorithms like PPO, you’ll need to pass in the extra values added during policy evaluation and postprocessing to `batch_builder.add_values()`, e.g., `logits`, `vf_preds`, `value_target`, and `advantages` for PPO. This is needed since the calculation of these values depends on the parameters of the behaviour policy, which RLlib does not have access to in the offline setting (in online training, these values are automatically added during policy evaluation).\n",
    "\n",
    "Note that for on-policy algorithms, you’ll also have to throw away experiences generated by prior versions of the policy. This greatly reduces sample efficiency, which is typically undesirable for offline training, but can make sense for certain applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mixing simulation and offline data**  \n",
    "RLlib supports multiplexing inputs from multiple input sources, including simulation. For example, in the following example we read 40% of our experiences from `/tmp/cartpole-out`, 30% from `hdfs:/archive/cartpole`, and the last 30% is produced via policy evaluation. Input sources are multiplexed using `np.random.choice`:\n",
    "\n",
    "```\n",
    "$ rllib train \\\n",
    "    --run=DQN \\\n",
    "    --env=CartPole-v0 \\\n",
    "    --config='{\n",
    "        \"input\": {\n",
    "            \"/tmp/cartpole-out\": 0.4,\n",
    "            \"hdfs:/archive/cartpole\": 0.3,\n",
    "            \"sampler\": 0.3,\n",
    "        },\n",
    "        \"explore\": false}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling I/O throughput**  \n",
    "Similar to scaling online training, you can scale offline I/O throughput by increasing the number of RLlib workers via the `num_workers` config. Each worker accesses offline storage independently in parallel, for linear scaling of I/O throughput. Within each read worker, files are chosen in random order for reads, but file contents are read sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Pipeline for Supervised Losses**\n",
    "You can also define supervised model losses over offline data. This requires defining a [custom model loss](https://ray.readthedocs.io/en/latest/rllib-models.html#supervised-model-losses). We provide a convenience function, `InputReader.tf_input_ops()`, that can be used to convert any input reader to a TF input pipeline. For example:\n",
    "See [custom_loss.py](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_loss.py) for a runnable example of using these TF input ops in a custom loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(self, policy_loss):\n",
    "    input_reader = JsonReader(\"/tmp/cartpole-out\")\n",
    "    # print(input_reader.next())  # if you want to access imperatively\n",
    "\n",
    "    input_ops = input_reader.tf_input_ops()\n",
    "    print(input_ops[\"obs\"])  # -> output Tensor shape=[None, 4]\n",
    "    print(input_ops[\"actions\"])  # -> output Tensor shape=[None]\n",
    "\n",
    "    supervised_loss = some_function_of(input_ops)\n",
    "    return policy_loss + supervised_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input API**  \n",
    "You can configure experience input for an agent using the following options:\n",
    "```\n",
    "    # Specify how to generate experiences:\n",
    "    #  - \"sampler\": generate experiences via online simulation (default)\n",
    "    #  - a local directory or file glob expression (e.g., \"/tmp/*.json\")\n",
    "    #  - a list of individual file paths/URIs (e.g., [\"/tmp/1.json\",\n",
    "    #    \"s3://bucket/2.json\"])\n",
    "    #  - a dict with string keys and sampling probabilities as values (e.g.,\n",
    "    #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).\n",
    "    #  - a function that returns a rllib.offline.InputReader\n",
    "    \"input\": \"sampler\",\n",
    "    # Specify how to evaluate the current policy. This only has an effect when\n",
    "    # reading offline experiences. Available options:\n",
    "    #  - \"wis\": the weighted step-wise importance sampling estimator.\n",
    "    #  - \"is\": the step-wise importance sampling estimator.\n",
    "    #  - \"simulation\": run the environment in the background, but use\n",
    "    #    this data for evaluation only and not for learning.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "    # Whether to run postprocess_trajectory() on the trajectory fragments from\n",
    "    # offline inputs. Note that postprocessing will be done using the *current*\n",
    "    # policy, not the *behavior* policy, which is typically undesirable for\n",
    "    # on-policy algorithms.\n",
    "    \"postprocess_inputs\": False,\n",
    "    # If positive, input batches will be shuffled via a sliding window buffer\n",
    "    # of this number of batches. Use this if the input data is not in random\n",
    "    # enough order. Input is delayed until the shuffle buffer is filled.\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "```\n",
    "\n",
    "\n",
    "**Output API**  \n",
    "You can configure experience output for an agent using the following options:\n",
    "```\n",
    "    # Specify where experiences should be saved:\n",
    "    #  - None: don't save any experiences\n",
    "    #  - \"logdir\" to save to the agent log dir\n",
    "    #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")\n",
    "    #  - a function that returns a rllib.offline.OutputWriter\n",
    "    \"output\": None,\n",
    "    # What sample batch columns to LZ4 compress in the output data.\n",
    "    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n",
    "    # Max output file size before rolling over to a new file.\n",
    "    \"output_max_file_size\": 64 * 1024 * 1024,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLlib Models, Preprocessors and Action Distributions \n",
    "see [documentation](https://ray.readthedocs.io/en/latest/rllib-models.html#rllib-models-preprocessors-and-action-distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RLlib Environments](https://ray.readthedocs.io/en/latest/rllib-env.html#rllib-environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RLlib Algorithms](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#rllib-algorithms)\n",
    "Description of the various algorithms already implemented and their default configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLlib [Examples](https://ray.readthedocs.io/en/latest/rllib-examples.html#rllib-examples)\n",
    "This includes many useful links to various examples of usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLlib [Package Reference](https://ray.readthedocs.io/en/latest/rllib-package-ref.html#rllib-package-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [Rllib Documentation](https://ray.readthedocs.io/en/latest/rllib.html)\n",
    "1. [Blog post : Functional RL with Keras and Tensorflow Eager](https://medium.com/riselab/functional-rl-with-keras-and-tensorflow-eager-7973f81d6345)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlib20d] *",
   "language": "python",
   "name": "conda-env-rlib20d-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.025px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
